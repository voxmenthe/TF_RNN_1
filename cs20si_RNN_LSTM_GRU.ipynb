{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A clean, no_frills character-level generative language model.\n",
    "Created by Danijar Hafner, edited by Chip Huyen\n",
    "for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "Based on Andrej Karpathy's blog: \n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'arvix_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "TEMPRATURE = 0.7\n",
    "LR = 0.003\n",
    "LEN_GENERATED = 300\n",
    "ITERATIONS = 101\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS/2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, int(overlap)):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    # cell = tf.contrib.rnn.GRUCell(hidden_size, reuse=True) # added 'reuse=True' JC\n",
    "    \n",
    "    in_state = tf.placeholder_with_default(\n",
    "            cell.zero_state(tf.shape(seq)[0], tf.float32), [None, hidden_size])\n",
    "    # this line to calculate the real length of seq\n",
    "    # all seq are padded to be of the same length which is NUM_STEPS\n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    return output, in_state, out_state\n",
    "\n",
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    # fully_connected is syntactic sugar for tf.matmul(w, output) + b\n",
    "    # it will create w and b for us\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1], labels=seq[:, 1:]))\n",
    "    # sample the next character from Maxwell-Boltzmann Distribution with temperature temp\n",
    "    # it works equally well without tf.exp\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0] \n",
    "    return loss, sample, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        iteration = global_step.eval()\n",
    "\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_inference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                saver.save(sess, 'checkpoints/char-rnn', iteration)\n",
    "            iteration += 1\n",
    "            if iteration > ITERATIONS:\n",
    "                break\n",
    "\n",
    "def online_inference(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "    \"\"\"\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        # for the first decoder step, the state is None\n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39. \n",
      "    Loss 9434.7236328125. Time 7.2893242835998535\n",
      "Tn,  e e e e e e e e e e e e e e e e e e e e e e e e e e e e  e  e e e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e   e  e  e  e  e  e  e  e      e                         \n",
      "Iter 79. \n",
      "    Loss 8284.513671875. Time 6.980986833572388\n",
      "Ta4 an an an an an an are the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "vocab = (\n",
    "        \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "seq = tf.placeholder(tf.int32, [None, None])\n",
    "temp = tf.placeholder(tf.float32)\n",
    "loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
