{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from integrated import BasicRNNCell, MultiRNNCell, dynamic_rnn\n",
    "%autosave 0\n",
    "\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import nn_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _linear(args, output_size, bias, bias_start=0.0):\n",
    "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "  Args:\n",
    "    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    bias: boolean, whether to add a bias term or not.\n",
    "    bias_start: starting value to initialize the bias; 0 by default.\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  \"\"\"\n",
    "  if args is None or (nest.is_sequence(args) and not args):\n",
    "    raise ValueError(\"`args` must be specified\")\n",
    "  if not nest.is_sequence(args):\n",
    "    args = [args]\n",
    "\n",
    "  # Calculate the total size of arguments on dimension 1.\n",
    "  total_arg_size = 0\n",
    "  shapes = [a.get_shape() for a in args]\n",
    "  for shape in shapes:\n",
    "    if shape.ndims != 2:\n",
    "      raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n",
    "    if shape[1].value is None:\n",
    "      raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n",
    "                       \"but saw %s\" % (shape, shape[1]))\n",
    "    else:\n",
    "      total_arg_size += shape[1].value\n",
    "\n",
    "  dtype = [a.dtype for a in args][0]\n",
    "\n",
    "  # Now the computation.\n",
    "  scope = vs.get_variable_scope()\n",
    "  with vs.variable_scope(scope) as outer_scope:\n",
    "    weights = vs.get_variable(\n",
    "        \"weights\", [total_arg_size, output_size], dtype=dtype)\n",
    "    \n",
    "    # either one argument or more\n",
    "    if len(args) == 1:\n",
    "      res = math_ops.matmul(args[0], weights)\n",
    "    else:\n",
    "      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n",
    "    \n",
    "    # add biases if they exist\n",
    "    if not bias:\n",
    "      return res\n",
    "    with vs.variable_scope(outer_scope) as inner_scope:\n",
    "      inner_scope.set_partitioner(None)\n",
    "      biases = vs.get_variable(\n",
    "          \"biases\", [output_size],\n",
    "          dtype=dtype,\n",
    "          initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\n",
    "        \n",
    "    return nn_ops.bias_add(res, biases)\n",
    "\n",
    "class RNNCell2(object):\n",
    "  \"\"\"\n",
    "  This definition of cell differs from the definition used in the literature.\n",
    "  In the literature, 'cell' refers to an object with a single scalar output.\n",
    "  This definition refers to a horizontal array of such units.\n",
    "  An RNN cell, in the most abstract setting, is anything that has\n",
    "  a state and performs some operation that takes a matrix of inputs.\n",
    "  This operation results in an output matrix with `self.output_size` columns.\n",
    "  If `self.state_size` is an integer, this operation also results in a new\n",
    "  state matrix with `self.state_size` columns.  If `self.state_size` is a\n",
    "  tuple of integers, then it results in a tuple of `len(state_size)` state\n",
    "  matrices, each with a column size corresponding to values in `state_size`.\n",
    "  \"\"\"\n",
    "  def __init__(self, num_units, activation=tf.tanh, reuse=None):\n",
    "    self._num_units = num_units\n",
    "    self._activation = activation\n",
    "    self._reuse = reuse\n",
    "    \n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run this RNN cell on inputs, starting from the given state.\n",
    "    Args:\n",
    "      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "      state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n",
    "        with shape `[batch_size x self.state_size]`.  Otherwise, if\n",
    "        `self.state_size` is a tuple of integers, this should be a tuple\n",
    "        with shapes `[batch_size x s] for s in self.state_size`.\n",
    "      scope: VariableScope for the created subgraph; defaults to class name.\n",
    "    Returns:\n",
    "      A pair containing:\n",
    "      - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n",
    "      - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
    "        the arity and shapes of `state`.\n",
    "    \"\"\"\n",
    "    \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "    \"\"\" U is W.hh in karpathy's code ???\"\"\"\n",
    "    output = self._activation(_linear([inputs, state], self._num_units, True))\n",
    "    return output, output\n",
    "\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    \"\"\"size(s) of state(s) used by this cell.\n",
    "    It can be represented by an Integer, a TensorShape or a tuple of Integers\n",
    "    or TensorShapes.\n",
    "    \"\"\"\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    \"\"\"Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n",
    "    return self._num_units\n",
    "\n",
    "  def zero_state(self, batch_size, dtype):\n",
    "    \"\"\"Return zero-filled state tensor(s).\n",
    "    Args:\n",
    "      batch_size: int, float, or unit Tensor representing the batch size.\n",
    "      dtype: the data type to use for the state.\n",
    "    Returns:\n",
    "      If `state_size` is an int or TensorShape, then the return value is a\n",
    "      `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n",
    "      If `state_size` is a nested list or tuple, then the return value is\n",
    "      a nested list or tuple (of the same structure) of `2-D` tensors with\n",
    "      the shapes `[batch_size x s]` for each s in `state_size`.\n",
    "    \"\"\"\n",
    "    with ops.name_scope(type(self).__name__ + \"ZeroState\", values=[batch_size]):\n",
    "      state_size = self.state_size\n",
    "      return _zero_state_tensors(state_size, batch_size, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(tensor,batch_size):\n",
    "    i = 0\n",
    "    while i < tensor.shape[0]//batch_size:\n",
    "        yield tensor[i:i+batch_size]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "num_units = 100\n",
    "n = 1000\n",
    "batch_size = 50\n",
    "X = np.arange(n*num_units).reshape((n,num_units))\n",
    "X_batches = X[:batch_size]\n",
    "#W = np.arange(9)[::-1].reshape((3,3))\n",
    "W = np.arange(num_units**2*2)[::-1].reshape((num_units*2,num_units))\n",
    "#h = np.ones(n*num_units).reshape((n,num_units)) # adding second dimension\n",
    "h = np.ones(batch_size*num_units).reshape((batch_size,num_units)) # adding second dimension\n",
    "B = np.arange(num_units)\n",
    "print(X)\n",
    "print()\n",
    "print(W)\n",
    "print()\n",
    "print(h)\n",
    "print()\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "gen = batch_generator(X,batch_size)\n",
    "x = next(gen)\n",
    "inputs = tf.Variable(initial_value=x,dtype=tf.float32)\n",
    "hidden_state = tf.Variable(initial_value=h,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "weights = []\n",
    "biases = []\n",
    "\n",
    "with vs.variable_scope(scope) as outer_scope:\n",
    "    # now becomes variable type which is a pointer to something inside the graph\n",
    "    weights = tf.get_variable(\"weights\",[num_units*2,num_units],dtype=tf.float32)\n",
    "    biases = tf.get_variable(\"biases\",[num_units],dtype=tf.float32)\n",
    "\n",
    "# now that we've set reuse no more variable creation\n",
    "scope.reuse_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer()\n",
    "    \n",
    "    out = sess.run(_linear([inputs,hidden_state], num_units, bias=True),feed_dict = {\n",
    "        weights: W,\n",
    "        inputs: x,\n",
    "        biases: B,\n",
    "        hidden_state: h\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "scope = vs.get_variable_scope()\n",
    "\n",
    "X = np.array([\"hello\"]*3)\n",
    "voc = np.unique(list(X))\n",
    "x = 'h' # I expect to get [0,1,0,0]\n",
    "ii = np.where(voc==x)\n",
    "z = tf.one_hot(ii[0],depth=len(voc))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot_generator(label,X):\n",
    "    X = np.array(X)\n",
    "    for x in X:\n",
    "        this_one = np.zeros(len(X))\n",
    "        this_one[np.where(X==label)] = 1\n",
    "    return this_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_generator('h',voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hello', 'hello', 'hello'], \n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X=='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_generator('h',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hello'], \n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for char in voc:\n",
    "    one_hot_generator(char,X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    cell_base = BasicRNNCell(100)\n",
    "    cell = MultiRNNCell([BasicRNNCell(100) for _ in range(num_layers)])\n",
    "    output, state = dynamic_rnn(cell, i, dtype=tf.float32)\n",
    "    print('\\nBasicRNN:\\nNumber of state outputs: {}\\n{}'.format(len(state), state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    cell_base = BasicRNNCell(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_units = 3\n",
    "n = 10\n",
    "\n",
    "X = np.array(list(\"hello\" *3))\n",
    "voc = np.unique(list(X))\n",
    "batch_size = len(voc)\n",
    "\n",
    "x = X[:batch_size] # 'h'\n",
    "x_one_hot = one_hot_generator(x,voc) # [0,1,0,0]\n",
    "\n",
    "W = np.arange(num_units**2*2)[::-1].reshape((num_units*2,num_units))\n",
    "h = np.ones(batch_size*num_units).reshape((batch_size,num_units)) # adding second dimension\n",
    "B = np.arange(num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a2c481f2e8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     out = sess.run(_linear([inputs,hidden_state], num_units, bias=True),feed_dict = {\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_one_hot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_state' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = x_one_hot\n",
    "\n",
    "with vs.variable_scope(scope) as outer_scope:\n",
    "    # now becomes variable type which is a pointer to something inside the graph\n",
    "    weights = tf.get_variable(\"weights\",[num_units*2,num_units],dtype=tf.float32)\n",
    "    biases = tf.get_variable(\"biases\",[num_units],dtype=tf.float32)\n",
    "\n",
    "# now that we've set reuse no more variable creation\n",
    "scope.reuse_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer()\n",
    "    \n",
    "    out = sess.run(_linear([inputs,hidden_state], num_units, bias=True),feed_dict = {\n",
    "        weights: W,\n",
    "        inputs: x_one_hot,\n",
    "        biases: B,\n",
    "        hidden_state: h\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
