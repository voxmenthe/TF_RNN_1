{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/jim-fleming/implementing-lstm-a-search-space-odyssey-7d50c3bacf93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM\n",
    "\n",
    "##### Based on simple version of LSTM defined as follows:\n",
    "\n",
    "###### Block input:\n",
    "$z^t = \\tanh(W_zX^t + R_zy^{t-1} + b_z$\n",
    "\n",
    "###### Input gate:\n",
    "$i^t = \\sigma(W_iX^t + R_iy^{t-1} + p_i\\bigodot c^{t-1} + b_i)$\n",
    "\n",
    "###### Forget gate:\n",
    "$f^t = \\sigma(W_fX^t + R_fy^{t-1} + p_f\\bigodot c^{t-1} + b_f)$\n",
    "\n",
    "###### Cell state:\n",
    "$c^t = i^t\\bigodot z^t + f^t \\bigodot c^{t-1}$\n",
    "\n",
    "###### Output gate:\n",
    "$o^t = \\sigma(W_oX^t+R_oy^{t-1}+p_o\\bigodot c^t+b_o)$\n",
    "\n",
    "###### Block output:\n",
    "$y^t = o^t\\bigodot \\tanh(c^t)$\n",
    "\n",
    "Note that from a performance perspective, this is a naïve implementation. If you look at the source for TensorFlow’s LSTMCell you’ll see that all of the cell inputs and states are concatenated together before doing any matrix multiplication. This is to improve performance, however, since we’re more interested in taking the LSTM apart, we’ll keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import nest\n",
    "\n",
    "def _zero_state_tensors(state_size, batch_size, dtype):\n",
    "  \"\"\"Create tensors of zeros based on state_size, batch_size, and dtype.\"\"\"\n",
    "  if nest.is_sequence(state_size):\n",
    "    state_size_flat = nest.flatten(state_size)\n",
    "    zeros_flat = [tf.zeros(tf.stack(_state_size_with_prefix(s, prefix=[batch_size])),dtype=dtype) for s in state_size_flat]\n",
    "    for s, z in zip(state_size_flat, zeros_flat):\n",
    "      z.set_shape(_state_size_with_prefix(s, prefix=[None]))\n",
    "    zeros = nest.pack_sequence_as(structure=state_size,\n",
    "                                  flat_sequence=zeros_flat)\n",
    "  else:\n",
    "    zeros_size = _state_size_with_prefix(state_size, prefix=[batch_size])\n",
    "    zeros = tf.zeros(array_ops.stack(zeros_size), dtype=dtype)\n",
    "    zeros.set_shape(_state_size_with_prefix(state_size, prefix=[None]))\n",
    "\n",
    "  return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaLSTMCell(object):\n",
    "    def __init__(self, num_blocks):\n",
    "        self._num_blocks = num_blocks\n",
    "        \n",
    "    @property\n",
    "    def input_size(self):\n",
    "        return self._num_blocks\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_blocks\n",
    "    \n",
    "\n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        \"\"\"Return zero-filled state tensor(s).\n",
    "        Args:\n",
    "          batch_size: int, float, or unit Tensor representing the batch size.\n",
    "          dtype: the data type to use for the state.\n",
    "        Returns:\n",
    "          If `state_size` is an int or TensorShape, then the return value is a\n",
    "          `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n",
    "          If `state_size` is a nested list or tuple, then the return value is\n",
    "          a nested list or tuple (of the same structure) of `2-D` tensors with\n",
    "          the shapes `[batch_size x s]` for each s in `state_size`.\n",
    "        \"\"\"\n",
    "        with ops.name_scope(type(self).__name__ + \"ZeroState\", values=[batch_size]):\n",
    "          state_size = self.state_size\n",
    "          return _zero_state_tensors(state_size, batch_size, dtype)\n",
    "    \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        with tf.variables_scope(scope or type(self).__name__):\n",
    "            initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "            \n",
    "            def get_variable(name, shape):\n",
    "                return tf.get_variable(name, shape, initializer=initializer, dtype=inputs.dtype)\n",
    "            \n",
    "            c_prev, y_prev = tf.split(state, 2, 1)\n",
    "            \n",
    "            # initialize all params using `get_variable` so we can reuse vars at each time-step\n",
    "            # instead of creating new params at each step\n",
    "            # also, all params are transposed from the paper's definitions to avoid additional graph operations\n",
    "            W_z = get_variable(\"W_z\", [self.input_size, self._num_blocks])\n",
    "            W_i = get_variable(\"W_i\", [self.input_size, self._num_blocks])\n",
    "            W_f = get_variable(\"W_f\", [self.input_size, self._num_blocks])\n",
    "            W_o = get_variable(\"W_o\", [self.input_size, self._num_blocks])\n",
    "            \n",
    "            R_z = get_variable(\"R_z\", [self.input_size, self._num_blocks])\n",
    "            R_i = get_variable(\"R_i\", [self.input_size, self._num_blocks])\n",
    "            R_f = get_variable(\"R_f\", [self.input_size, self._num_blocks])\n",
    "            R_o = get_variable(\"R_o\", [self.input_size, self._num_blocks])\n",
    "            \n",
    "            b_z = get_variable(\"b_z\", [self.input_size, self._num_blocks])\n",
    "            b_i = get_variable(\"b_i\", [self.input_size, self._num_blocks])\n",
    "            b_f = get_variable(\"b_f\", [self.input_size, self._num_blocks])\n",
    "            b_o = get_variable(\"b_o\", [self.input_size, self._num_blocks])\n",
    "            \n",
    "            p_i = get_variable(\"p_i\", [self.input_size, self._num_blocks])\n",
    "            p_f = get_variable(\"p_f\", [self.input_size, self._num_blocks])\n",
    "            p_o = get_variable(\"p_o\", [self.input_size, self._num_blocks])\n",
    "            \n",
    "            # define each equation as operations in the graph\n",
    "            # many have reversed inputs so that matmuls produce correct dimensionality\n",
    "            z = tf.tanh(tf.matmul(inputs, W_z) + tf.matmul(y_prev, R_z) + b_z)\n",
    "            i = tf.sigmoid(tf.matmul(inputs, W_i) + tf.matmul(y_prev, R_i) + tf.mul(c_prev, p_i) + b_i)\n",
    "            f = tf.sigmoid(tf.matmul(inputs, W_f) + tf.matmul(y_prev, R_f), + tf.mul(c_prev, p_f) + b_f)\n",
    "            c = tf.mul(i, z) + tf.mul(f, c_prev)\n",
    "            o = tf.sigmoid(tf.matmul(inputs, W_o) + tf.matmul(y_prev, R_o) + tf.mul(c, p_o) + b_o)\n",
    "            y = tf.mul(tf.tanh(c), o)\n",
    "            \n",
    "            return y, tf.concat([c,y],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
