{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.models.rnn.seq2seq import sequence_loss_by_example\n",
    "\n",
    "from DropoutWrapper import DropoutWrapper\n",
    "from MultiRNNCell2 import MultiRNNCell2\n",
    "\n",
    "# parses the dataset\n",
    "import ptb_reader\n",
    "\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/1A_Tensorflow/AA_RNN_Practice/TF_RNN_1/my_lstm_odyssey\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define artifact directories where results from the session can be saved\n",
    "model_path = os.environ.get('MODEL_PATH', 'models/')\n",
    "checkpoint_path = os.environ.get('CHECKPOINT_PATH', 'checkpoints/')\n",
    "summary_path = os.environ.get('SUMMARY_PATH', 'logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/1A_Tensorflow/AA_RNN_Practice/TF_RNN_1/my_lstm_odyssey\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_data, valid_data, test_data, _ = ptb_reader.ptb_raw_data(data_path=\"ptb\")\n",
    "\n",
    "def write_csv(arr, path):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Adapted to support sampled_softmax loss function, which accepts activations\n",
    "# instead of logits.\n",
    "def sequence_loss_by_example(inputs, targets, weights, loss_function,\n",
    "                             average_across_timesteps=True, name=None):\n",
    "  \"\"\"Sampled softmax loss for a sequence of inputs (per example).\n",
    "  Args:\n",
    "    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\n",
    "    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "    loss_function: Sampled softmax function (inputs, labels) -> loss\n",
    "    average_across_timesteps: If set, divide the returned cost by the total\n",
    "      label weight.\n",
    "    name: Optional name for this operation, default: 'sequence_loss_by_example'.\n",
    "  Returns:\n",
    "    1D batch-sized float Tensor: The log-perplexity for each sequence.\n",
    "  Raises:\n",
    "    ValueError: If len(inputs) is different from len(targets) or len(weights).\n",
    "  \"\"\"\n",
    "  if len(targets) != len(inputs) or len(weights) != len(inputs):\n",
    "    raise ValueError('Lengths of logits, weights, and targets must be the same '\n",
    "                     '%d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n",
    "  with tf.name_scope(values=inputs + targets + weights, name=name,\n",
    "                     default_name='sequence_loss_by_example'):\n",
    "    log_perp_list = []\n",
    "    for inp, target, weight in zip(inputs, targets, weights):\n",
    "      crossent = loss_function(inp, target)\n",
    "      log_perp_list.append(crossent * weight)\n",
    "    log_perps = tf.add_n(log_perp_list)\n",
    "    if average_across_timesteps:\n",
    "      total_size = tf.add_n(weights)\n",
    "      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "      log_perps /= total_size\n",
    "  return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.12.1 version\n",
    "def sequence_loss_by_example(logits, targets, weights,\n",
    "                             average_across_timesteps=True,\n",
    "                             softmax_loss_function=None, name=None):\n",
    "  \"\"\"Weighted cross-entropy loss for a sequence of logits (per example).\n",
    "\n",
    "  Args:\n",
    "    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n",
    "    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "    average_across_timesteps: If set, divide the returned cost by the total\n",
    "      label weight.\n",
    "    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n",
    "      to be used instead of the standard softmax (the default if this is None).\n",
    "    name: Optional name for this operation, default: \"sequence_loss_by_example\".\n",
    "\n",
    "  Returns:\n",
    "    1D batch-sized float Tensor: The log-perplexity for each sequence.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If len(logits) is different from len(targets) or len(weights).\n",
    "  \"\"\"\n",
    "  if len(targets) != len(logits) or len(weights) != len(logits):\n",
    "    raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n",
    "                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n",
    "  with tf.name_scope(name, \"sequence_loss_by_example\",\n",
    "                      logits + targets + weights):\n",
    "    log_perp_list = []\n",
    "    for logit, target, weight in zip(logits, targets, weights):\n",
    "      if softmax_loss_function is None:\n",
    "        # TODO(irving,ebrevdo): This reshape is needed because\n",
    "        # sequence_loss_by_example is called with scalars sometimes, which\n",
    "        # violates our general scalar strictness policy.\n",
    "        target = tf.reshape(target, [-1])\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logit, labels=target)\n",
    "      else:\n",
    "        crossent = softmax_loss_function(logit, target)\n",
    "      log_perp_list.append(crossent * weight)\n",
    "    log_perps = tf.add_n(log_perp_list)\n",
    "    if average_across_timesteps:\n",
    "      total_size = tf.add_n(weights)\n",
    "      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "      log_perps /= total_size\n",
    "  return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tf.models.rnn.seq2seq import sequence_loss_by_example\n",
    "\n",
    "class PTBModel(object):\n",
    "    def __init__(self, CellType, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "        \n",
    "        lstm_cell = CellType(size)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob = config.keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # initalizer used for reusable variable initializer (see 'get_variable')\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "        \n",
    "#        with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, size], initializer=initializer)\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "            \n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "            \n",
    "        outputs = []\n",
    "        states = []\n",
    "        state = self.initial_state\n",
    "        \n",
    "        with tf.variable_scope(\"RNN\", initializer=initializer):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    \n",
    "                inputs_slice = inputs[:,time_step,:]\n",
    "                (cell_output, state) = cell(inputs_slice, state)\n",
    "                \n",
    "                outputs.append(cell_output)\n",
    "                states.append(state)\n",
    "                \n",
    "        self.final_state = states[-1]\n",
    "        \n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, size])\n",
    "        w = tf.get_variable(\"softmax_w\",\n",
    "                           [size, vocab_size],\n",
    "                           initializer=initializer)\n",
    "        b = tf.get_variable(\"softmax_b\", [vocab_size], initializer=initializer)\n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(output, w, b) # compute logits for loss\n",
    "        targets = tf.reshape(self.targets, [-1]) # reshape target outputs\n",
    "        weights = tf.ones([batch_size * num_steps]) # used to scale the loss average\n",
    "        \n",
    "        # computes loss and performs softmax on our fully-connected output layer\n",
    "#         def sequence_loss_by_example(inputs, targets, weights, loss_function,\n",
    "#                              average_across_timesteps=True, name=None):\n",
    "        \n",
    "        loss = sequence_loss_by_example([logits], [targets], [weights], vocab_size)\n",
    "        self.cost = cost = tf.div(tf.reduce_sum(loss), batch_size, name=\"cost\")\n",
    "        \n",
    "        if is_training:\n",
    "            # set up learning rate variable to decay\n",
    "            self.lr = tf.Variable(0.98, trainable=False)\n",
    "            #self.lr = tf.Variable(0.0001, trainable=False)\n",
    "            \n",
    "            # define training operation and clip the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate=self.lr)\n",
    "            \n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train\")\n",
    "        else:\n",
    "            # if this model isn't for training (i.e. testing/validation) then we don't do anything here\n",
    "            self.train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(sess, model, data, verbose=False):\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # accumulated counts\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    \n",
    "    # initial RNN state\n",
    "    # print(model.initial_state,type(model.initial_state))\n",
    "    # state = model.initial_state.eval()\n",
    "    state = tf.get_default_session().run(model.initial_state)\n",
    "    \n",
    "    # example of how to change:\n",
    "    # state = cell.zero_state(batchsize, tf.float32).eval()\n",
    "    # state = tf.get_default_session().run(cell.zero_state(batchsize, tf.float32))\n",
    "    \n",
    "    for step, (x,y) in enumerate(ptb_reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed_dict={\n",
    "            model.input_data: x,\n",
    "            model.targets: y,\n",
    "            model.initial_state: state\n",
    "        })\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "        \n",
    "        perplexity = np.exp(costs / iters)\n",
    "        \n",
    "        if verbose and step %10 == 0:\n",
    "            progress = (step / epoch_size) * 100\n",
    "            wps = iters * model.batch_size / (time.time() - start_time)\n",
    "            print(\"Progress:  %.1f%% Perpexity: %.2f (Cost: %.2f) Speed: %.0f wps\" % (progress, perplexity, cost, wps))\n",
    "            \n",
    "        return (costs / iters), perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    batch_size = 20\n",
    "    num_steps = 35 # number of unrolled time steps\n",
    "    hidden_size = 450 # number of blocks in an LSTM cell\n",
    "    vocab_size = 10000\n",
    "    max_grad_norm = 5 # maximum gradient for clipping\n",
    "    init_scale = 0.05 # scale between -0.1 and 0.1 for all random initialization\n",
    "    keep_prob = 0.5 # dropout probability\n",
    "    num_layers = 2 # number of LSTM layers\n",
    "    #learning_rate = 0.0001 # 1.0\n",
    "    learning_rate = 0.001 # not getting used at the moment\n",
    "    lr_decay = 0.985\n",
    "    lr_decay_epoch_offset = 6 # don't decay until after the Nth epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default settings for training\n",
    "train_config = Config()\n",
    "\n",
    "# our evaluation runs (validation and testing), use a batch size and time_step of one\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "# number of epochs to perform over the training data\n",
    "num_epochs = 39\n",
    "\n",
    "import vanilla, cifg, MILSTM\n",
    "\n",
    "cell_types = {\n",
    "    'vanilla': vanilla.VanillaLSTMCell,\n",
    "    'cifg': cifg.CIFGLSTMCell,\n",
    "    'milstm': MILSTM.MILSTMCell\n",
    "#     'nig': NIGLSTMCell,\n",
    "#     'nfg': NFGLSTMCell,\n",
    "#     'nog': NOGLSTMCell,\n",
    "#     'niaf': NIAFLSTMCell,\n",
    "#     'noaf': NOAFLSTMCell,\n",
    "#     'np': NPLSTMCell,\n",
    "#     'fgr': FGRLSTMCell,\n",
    "}\n",
    "\n",
    "#model_name = \"vanilla\"\n",
    "#model_name = \"cifg\"\n",
    "model_name = \"milstm\"\n",
    "\n",
    "CellType = cell_types[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 9934.50 (Cost: 322.13) Speed: 130 wps\n",
      "Epoch: 1 Training Perplexity: 9934.50 (Cost: 9.20)\n",
      "Epoch: 1 Validation Perplexity: 5319.52 (Cost: 8.58)\n",
      "Epoch: 2 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 5021.37 (Cost: 298.25) Speed: 310 wps\n",
      "Epoch: 2 Training Perplexity: 5021.37 (Cost: 8.52)\n",
      "Epoch: 2 Validation Perplexity: 4536.69 (Cost: 8.42)\n",
      "Epoch: 3 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 3898.83 (Cost: 289.40) Speed: 306 wps\n",
      "Epoch: 3 Training Perplexity: 3898.83 (Cost: 8.27)\n",
      "Epoch: 3 Validation Perplexity: 3305.66 (Cost: 8.10)\n",
      "Epoch: 4 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 3276.45 (Cost: 283.31) Speed: 271 wps\n",
      "Epoch: 4 Training Perplexity: 3276.45 (Cost: 8.09)\n",
      "Epoch: 4 Validation Perplexity: 2628.50 (Cost: 7.87)\n",
      "Epoch: 5 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 2259.64 (Cost: 270.30) Speed: 302 wps\n",
      "Epoch: 5 Training Perplexity: 2259.64 (Cost: 7.72)\n",
      "Epoch: 5 Validation Perplexity: 2593.02 (Cost: 7.86)\n",
      "Epoch: 6 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 2111.14 (Cost: 267.92) Speed: 301 wps\n",
      "Epoch: 6 Training Perplexity: 2111.14 (Cost: 7.65)\n",
      "Epoch: 6 Validation Perplexity: 3681.01 (Cost: 8.21)\n",
      "Epoch: 7 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 2606.18 (Cost: 275.30) Speed: 322 wps\n",
      "Epoch: 7 Training Perplexity: 2606.18 (Cost: 7.87)\n",
      "Epoch: 7 Validation Perplexity: 1981.19 (Cost: 7.59)\n",
      "Epoch: 8 Learning Rate: 0.96530\n",
      "Progress:  0.0% Perpexity: 1596.93 (Cost: 258.15) Speed: 324 wps\n",
      "Epoch: 8 Training Perplexity: 1596.93 (Cost: 7.38)\n",
      "Epoch: 8 Validation Perplexity: 1649.34 (Cost: 7.41)\n",
      "Epoch: 9 Learning Rate: 0.95082\n",
      "Progress:  0.0% Perpexity: 1050.14 (Cost: 243.48) Speed: 330 wps\n",
      "Epoch: 9 Training Perplexity: 1050.14 (Cost: 6.96)\n",
      "Epoch: 9 Validation Perplexity: 1828.45 (Cost: 7.51)\n",
      "Epoch: 10 Learning Rate: 0.93656\n",
      "Progress:  0.0% Perpexity: 1140.07 (Cost: 246.36) Speed: 313 wps\n",
      "Epoch: 10 Training Perplexity: 1140.07 (Cost: 7.04)\n",
      "Epoch: 10 Validation Perplexity: 30336.68 (Cost: 10.32)\n",
      "Epoch: 11 Learning Rate: 0.92251\n",
      "Progress:  0.0% Perpexity: 13450.45 (Cost: 332.74) Speed: 313 wps\n",
      "Epoch: 11 Training Perplexity: 13450.45 (Cost: 9.51)\n",
      "Epoch: 11 Validation Perplexity: 2756.91 (Cost: 7.92)\n",
      "Epoch: 12 Learning Rate: 0.90867\n",
      "Progress:  0.0% Perpexity: 1646.79 (Cost: 259.23) Speed: 328 wps\n",
      "Epoch: 12 Training Perplexity: 1646.79 (Cost: 7.41)\n",
      "Epoch: 12 Validation Perplexity: 2228.80 (Cost: 7.71)\n",
      "Epoch: 13 Learning Rate: 0.89504\n",
      "Progress:  0.0% Perpexity: 1239.88 (Cost: 249.30) Speed: 302 wps\n",
      "Epoch: 13 Training Perplexity: 1239.88 (Cost: 7.12)\n",
      "Epoch: 13 Validation Perplexity: 3972.13 (Cost: 8.29)\n",
      "Epoch: 14 Learning Rate: 0.88162\n",
      "Progress:  0.0% Perpexity: 1684.02 (Cost: 260.01) Speed: 306 wps\n",
      "Epoch: 14 Training Perplexity: 1684.02 (Cost: 7.43)\n",
      "Epoch: 14 Validation Perplexity: 2063.04 (Cost: 7.63)\n",
      "Epoch: 15 Learning Rate: 0.86839\n",
      "Progress:  0.0% Perpexity: 1117.60 (Cost: 245.66) Speed: 303 wps\n",
      "Epoch: 15 Training Perplexity: 1117.60 (Cost: 7.02)\n",
      "Epoch: 15 Validation Perplexity: 1708.17 (Cost: 7.44)\n",
      "Epoch: 16 Learning Rate: 0.85537\n",
      "Progress:  0.0% Perpexity: 853.40 (Cost: 236.22) Speed: 298 wps\n",
      "Epoch: 16 Training Perplexity: 853.40 (Cost: 6.75)\n",
      "Epoch: 16 Validation Perplexity: 4447.76 (Cost: 8.40)\n",
      "Epoch: 17 Learning Rate: 0.84254\n",
      "Progress:  0.0% Perpexity: 1573.15 (Cost: 257.63) Speed: 290 wps\n",
      "Epoch: 17 Training Perplexity: 1573.15 (Cost: 7.36)\n",
      "Epoch: 17 Validation Perplexity: 2367.93 (Cost: 7.77)\n",
      "Epoch: 18 Learning Rate: 0.82990\n",
      "Progress:  0.0% Perpexity: 1090.12 (Cost: 244.79) Speed: 313 wps\n",
      "Epoch: 18 Training Perplexity: 1090.12 (Cost: 6.99)\n",
      "Epoch: 18 Validation Perplexity: 1664.16 (Cost: 7.42)\n",
      "Epoch: 19 Learning Rate: 0.81745\n",
      "Progress:  0.0% Perpexity: 733.75 (Cost: 230.94) Speed: 320 wps\n",
      "Epoch: 19 Training Perplexity: 733.75 (Cost: 6.60)\n",
      "Epoch: 19 Validation Perplexity: 9815.63 (Cost: 9.19)\n",
      "Epoch: 20 Learning Rate: 0.80519\n",
      "Progress:  0.0% Perpexity: 2880.93 (Cost: 278.81) Speed: 318 wps\n",
      "Epoch: 20 Training Perplexity: 2880.93 (Cost: 7.97)\n",
      "Epoch: 20 Validation Perplexity: 2359.22 (Cost: 7.77)\n",
      "Epoch: 21 Learning Rate: 0.79311\n",
      "Progress:  0.0% Perpexity: 929.03 (Cost: 239.20) Speed: 310 wps\n",
      "Epoch: 21 Training Perplexity: 929.03 (Cost: 6.83)\n",
      "Epoch: 21 Validation Perplexity: 2205.45 (Cost: 7.70)\n",
      "Epoch: 22 Learning Rate: 0.78121\n",
      "Progress:  0.0% Perpexity: 812.17 (Cost: 234.49) Speed: 305 wps\n",
      "Epoch: 22 Training Perplexity: 812.17 (Cost: 6.70)\n",
      "Epoch: 22 Validation Perplexity: 3410.07 (Cost: 8.13)\n",
      "Epoch: 23 Learning Rate: 0.76950\n",
      "Progress:  0.0% Perpexity: 1102.27 (Cost: 245.18) Speed: 295 wps\n",
      "Epoch: 23 Training Perplexity: 1102.27 (Cost: 7.01)\n",
      "Epoch: 23 Validation Perplexity: 2298.96 (Cost: 7.74)\n",
      "Epoch: 24 Learning Rate: 0.75795\n",
      "Progress:  0.0% Perpexity: 906.95 (Cost: 238.35) Speed: 308 wps\n",
      "Epoch: 24 Training Perplexity: 906.95 (Cost: 6.81)\n",
      "Epoch: 24 Validation Perplexity: 1509.48 (Cost: 7.32)\n",
      "Epoch: 25 Learning Rate: 0.74658\n",
      "Progress:  0.0% Perpexity: 529.85 (Cost: 219.54) Speed: 305 wps\n",
      "Epoch: 25 Training Perplexity: 529.85 (Cost: 6.27)\n",
      "Epoch: 25 Validation Perplexity: 2702.57 (Cost: 7.90)\n",
      "Epoch: 26 Learning Rate: 0.73538\n",
      "Progress:  0.0% Perpexity: 769.08 (Cost: 232.58) Speed: 304 wps\n",
      "Epoch: 26 Training Perplexity: 769.08 (Cost: 6.65)\n",
      "Epoch: 26 Validation Perplexity: 2585.28 (Cost: 7.86)\n",
      "Epoch: 27 Learning Rate: 0.72435\n",
      "Progress:  0.0% Perpexity: 881.14 (Cost: 237.34) Speed: 304 wps\n",
      "Epoch: 27 Training Perplexity: 881.14 (Cost: 6.78)\n",
      "Epoch: 27 Validation Perplexity: 1922.23 (Cost: 7.56)\n",
      "Epoch: 28 Learning Rate: 0.71349\n",
      "Progress:  0.0% Perpexity: 612.31 (Cost: 224.60) Speed: 320 wps\n",
      "Epoch: 28 Training Perplexity: 612.31 (Cost: 6.42)\n",
      "Epoch: 28 Validation Perplexity: 1721.36 (Cost: 7.45)\n",
      "Epoch: 29 Learning Rate: 0.70279\n",
      "Progress:  0.0% Perpexity: 574.42 (Cost: 222.37) Speed: 325 wps\n",
      "Epoch: 29 Training Perplexity: 574.42 (Cost: 6.35)\n",
      "Epoch: 29 Validation Perplexity: 3367.30 (Cost: 8.12)\n",
      "Epoch: 30 Learning Rate: 0.69224\n",
      "Progress:  0.0% Perpexity: 705.43 (Cost: 229.56) Speed: 294 wps\n",
      "Epoch: 30 Training Perplexity: 705.43 (Cost: 6.56)\n",
      "Epoch: 30 Validation Perplexity: 2565.04 (Cost: 7.85)\n",
      "Epoch: 31 Learning Rate: 0.68186\n",
      "Progress:  0.0% Perpexity: 516.00 (Cost: 218.61) Speed: 302 wps\n",
      "Epoch: 31 Training Perplexity: 516.00 (Cost: 6.25)\n",
      "Epoch: 31 Validation Perplexity: 5155.93 (Cost: 8.55)\n",
      "Epoch: 32 Learning Rate: 0.67163\n",
      "Progress:  0.0% Perpexity: 1217.43 (Cost: 248.66) Speed: 313 wps\n",
      "Epoch: 32 Training Perplexity: 1217.43 (Cost: 7.10)\n",
      "Epoch: 32 Validation Perplexity: 2103.94 (Cost: 7.65)\n",
      "Epoch: 33 Learning Rate: 0.66156\n",
      "Progress:  0.0% Perpexity: 576.74 (Cost: 222.51) Speed: 312 wps\n",
      "Epoch: 33 Training Perplexity: 576.74 (Cost: 6.36)\n",
      "Epoch: 33 Validation Perplexity: 3646.01 (Cost: 8.20)\n",
      "Epoch: 34 Learning Rate: 0.65164\n",
      "Progress:  0.0% Perpexity: 768.33 (Cost: 232.55) Speed: 309 wps\n",
      "Epoch: 34 Training Perplexity: 768.33 (Cost: 6.64)\n",
      "Epoch: 34 Validation Perplexity: 2499.91 (Cost: 7.82)\n",
      "Epoch: 35 Learning Rate: 0.64186\n",
      "Progress:  0.0% Perpexity: 644.04 (Cost: 226.37) Speed: 312 wps\n",
      "Epoch: 35 Training Perplexity: 644.04 (Cost: 6.47)\n",
      "Epoch: 35 Validation Perplexity: 2438.41 (Cost: 7.80)\n",
      "Epoch: 36 Learning Rate: 0.63223\n",
      "Progress:  0.0% Perpexity: 582.34 (Cost: 222.85) Speed: 310 wps\n",
      "Epoch: 36 Training Perplexity: 582.34 (Cost: 6.37)\n",
      "Epoch: 36 Validation Perplexity: 2076.51 (Cost: 7.64)\n",
      "Epoch: 37 Learning Rate: 0.62275\n",
      "Progress:  0.0% Perpexity: 514.85 (Cost: 218.54) Speed: 332 wps\n",
      "Epoch: 37 Training Perplexity: 514.85 (Cost: 6.24)\n",
      "Epoch: 37 Validation Perplexity: 2058.98 (Cost: 7.63)\n",
      "Epoch: 38 Learning Rate: 0.61341\n",
      "Progress:  0.0% Perpexity: 420.62 (Cost: 211.46) Speed: 300 wps\n",
      "Epoch: 38 Training Perplexity: 420.62 (Cost: 6.04)\n",
      "Epoch: 38 Validation Perplexity: 3404.62 (Cost: 8.13)\n",
      "Epoch: 39 Learning Rate: 0.60421\n",
      "Progress:  0.0% Perpexity: 629.18 (Cost: 225.55) Speed: 325 wps\n",
      "Epoch: 39 Training Perplexity: 629.18 (Cost: 6.44)\n",
      "Epoch: 39 Validation Perplexity: 2228.33 (Cost: 7.71)\n",
      "Test Perplexity: 333.83 (Cost: 5.81)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # define our training model\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        train_model = PTBModel(CellType, is_training=True, config=train_config)\n",
    "        \n",
    "    # we create a separate model for validation and testing to alter the batch_size and time steps\n",
    "    # reuse = True reuses variables from the previously defined 'train_model'\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        valid_model = PTBModel(CellType, is_training=False, config=train_config)\n",
    "        test_model = PTBModel(CellType, is_training=False, config=eval_config)\n",
    "\n",
    "    # create a saver instance to restore from the checkpoint\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    # initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # save the graph definition as a protobuf file\n",
    "    tf.train.write_graph(sess.graph_def, model_path, '%s.pb'.format(model_name), as_text=False)\n",
    "\n",
    "    train_costs = []\n",
    "    train_perps = []\n",
    "    valid_costs = []\n",
    "    valid_perps = []\n",
    "    \n",
    "    # set initial learning rate - can I do this here? so far throws an error\n",
    "    #train_model.lr = train_config.learning_rate\n",
    "    #train_model.lr = tf.Variable(train_config.learning_rate, trainable=False)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        # decay learning rate\n",
    "        if i > train_config.lr_decay_epoch_offset:\n",
    "            train_model.lr = train_model.lr * train_config.lr_decay\n",
    "        \n",
    "        print(\"Epoch: %d Learning Rate: %.5f\" % (i + 1, sess.run(train_model.lr)))\n",
    "\n",
    "        # run training pass\n",
    "        train_cost, train_perp = run_epoch(sess, train_model, train_data, verbose=True)\n",
    "        print(\"Epoch: %i Training Perplexity: %.2f (Cost: %.2f)\" % (i + 1,  train_perp, train_cost))\n",
    "        train_costs.append(train_cost)\n",
    "        train_perps.append(train_perp)\n",
    "\n",
    "        # run validation pass\n",
    "        valid_cost, valid_perp = run_epoch(sess, valid_model, valid_data)\n",
    "        print(\"Epoch: %i Validation Perplexity: %.2f (Cost: %.2f)\" % (i + 1, valid_perp, valid_cost))\n",
    "        valid_costs.append(valid_cost)\n",
    "        valid_perps.append(valid_perp)\n",
    "\n",
    "        # save(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\n",
    "        saver.save(sess, checkpoint_path + 'checkpoint', global_step=i,latest_filename=None)\n",
    "\n",
    "    # run test pass\n",
    "    test_cost, test_perp = run_epoch(sess, test_model, test_data)\n",
    "    print(\"Test Perplexity: %.2f (Cost: %.2f)\" % (test_perp, test_cost))\n",
    "\n",
    "    write_csv(train_costs, os.path.join(summary_path, \"train_costs.csv\"))\n",
    "    write_csv(train_perps, os.path.join(summary_path, \"train_perps.csv\"))\n",
    "    write_csv(valid_costs, os.path.join(summary_path, \"valid_costs.csv\"))\n",
    "    write_csv(valid_perps, os.path.join(summary_path, \"valid_perps.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
