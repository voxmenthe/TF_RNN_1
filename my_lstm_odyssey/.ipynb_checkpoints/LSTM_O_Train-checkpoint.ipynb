{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.models.rnn.seq2seq import sequence_loss_by_example\n",
    "\n",
    "from DropoutWrapper import DropoutWrapper\n",
    "from MultiRNNCell2 import MultiRNNCell2\n",
    "\n",
    "# parses the dataset\n",
    "import ptb_reader\n",
    "\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/1A_Tensorflow/AA_RNN_Practice/TF_RNN_1/my_lstm_odyssey\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define artifact directories where results from the session can be saved\n",
    "model_path = os.environ.get('MODEL_PATH', 'models/')\n",
    "checkpoint_path = os.environ.get('CHECKPOINT_PATH', 'checkpoints/')\n",
    "summary_path = os.environ.get('SUMMARY_PATH', 'logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/1A_Tensorflow/AA_RNN_Practice/TF_RNN_1/my_lstm_odyssey\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_data, valid_data, test_data, _ = ptb_reader.ptb_raw_data(data_path=\"ptb\")\n",
    "\n",
    "# note this throws an error if the UTF-8 encoding in your environment isn't set just right\n",
    "def write_csv(arr, path):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Adapted to support sampled_softmax loss function, which accepts activations\n",
    "# instead of logits.\n",
    "def sequence_loss_by_example(inputs, targets, weights, loss_function,\n",
    "                             average_across_timesteps=True, name=None):\n",
    "  \"\"\"Sampled softmax loss for a sequence of inputs (per example).\n",
    "  Args:\n",
    "    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\n",
    "    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "    loss_function: Sampled softmax function (inputs, labels) -> loss\n",
    "    average_across_timesteps: If set, divide the returned cost by the total\n",
    "      label weight.\n",
    "    name: Optional name for this operation, default: 'sequence_loss_by_example'.\n",
    "  Returns:\n",
    "    1D batch-sized float Tensor: The log-perplexity for each sequence.\n",
    "  Raises:\n",
    "    ValueError: If len(inputs) is different from len(targets) or len(weights).\n",
    "  \"\"\"\n",
    "  if len(targets) != len(inputs) or len(weights) != len(inputs):\n",
    "    raise ValueError('Lengths of logits, weights, and targets must be the same '\n",
    "                     '%d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n",
    "  with tf.name_scope(values=inputs + targets + weights, name=name,\n",
    "                     default_name='sequence_loss_by_example'):\n",
    "    log_perp_list = []\n",
    "    for inp, target, weight in zip(inputs, targets, weights):\n",
    "      crossent = loss_function(inp, target)\n",
    "      log_perp_list.append(crossent * weight)\n",
    "    log_perps = tf.add_n(log_perp_list)\n",
    "    if average_across_timesteps:\n",
    "      total_size = tf.add_n(weights)\n",
    "      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "      log_perps /= total_size\n",
    "  return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.12.1 version\n",
    "def sequence_loss_by_example(logits, targets, weights,\n",
    "                             average_across_timesteps=True,\n",
    "                             softmax_loss_function=None, name=None):\n",
    "  \"\"\"Weighted cross-entropy loss for a sequence of logits (per example).\n",
    "\n",
    "  Args:\n",
    "    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n",
    "    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "    average_across_timesteps: If set, divide the returned cost by the total\n",
    "      label weight.\n",
    "    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n",
    "      to be used instead of the standard softmax (the default if this is None).\n",
    "    name: Optional name for this operation, default: \"sequence_loss_by_example\".\n",
    "\n",
    "  Returns:\n",
    "    1D batch-sized float Tensor: The log-perplexity for each sequence.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If len(logits) is different from len(targets) or len(weights).\n",
    "  \"\"\"\n",
    "  if len(targets) != len(logits) or len(weights) != len(logits):\n",
    "    raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n",
    "                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n",
    "  with tf.name_scope(name, \"sequence_loss_by_example\",\n",
    "                      logits + targets + weights):\n",
    "    log_perp_list = []\n",
    "    for logit, target, weight in zip(logits, targets, weights):\n",
    "      if softmax_loss_function is None:\n",
    "        # TODO(irving,ebrevdo): This reshape is needed because\n",
    "        # sequence_loss_by_example is called with scalars sometimes, which\n",
    "        # violates our general scalar strictness policy.\n",
    "        target = tf.reshape(target, [-1])\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logit, labels=target)\n",
    "      else:\n",
    "        crossent = softmax_loss_function(logit, target)\n",
    "      log_perp_list.append(crossent * weight)\n",
    "    log_perps = tf.add_n(log_perp_list)\n",
    "    if average_across_timesteps:\n",
    "      total_size = tf.add_n(weights)\n",
    "      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "      log_perps /= total_size\n",
    "  return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tf.models.rnn.seq2seq import sequence_loss_by_example\n",
    "\n",
    "class PTBModel(object):\n",
    "    def __init__(self, CellType, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "        \n",
    "        lstm_cell = CellType(size)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob = config.keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # initalizer used for reusable variable initializer (see 'get_variable')\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "        \n",
    "#        with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, size], initializer=initializer)\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "            \n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "            \n",
    "        outputs = []\n",
    "        states = []\n",
    "        state = self.initial_state\n",
    "        \n",
    "        with tf.variable_scope(\"RNN\", initializer=initializer):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    \n",
    "                inputs_slice = inputs[:,time_step,:]\n",
    "                (cell_output, state) = cell(inputs_slice, state)\n",
    "                \n",
    "                outputs.append(cell_output)\n",
    "                states.append(state)\n",
    "                \n",
    "        self.final_state = states[-1]\n",
    "        \n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, size])\n",
    "        w = tf.get_variable(\"softmax_w\",\n",
    "                           [size, vocab_size],\n",
    "                           initializer=initializer)\n",
    "        b = tf.get_variable(\"softmax_b\", [vocab_size], initializer=initializer)\n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(output, w, b) # compute logits for loss\n",
    "        targets = tf.reshape(self.targets, [-1]) # reshape target outputs\n",
    "        weights = tf.ones([batch_size * num_steps]) # used to scale the loss average\n",
    "        \n",
    "        # computes loss and performs softmax on our fully-connected output layer\n",
    "#         def sequence_loss_by_example(inputs, targets, weights, loss_function,\n",
    "#                              average_across_timesteps=True, name=None):\n",
    "        \n",
    "        loss = sequence_loss_by_example([logits], [targets], [weights], vocab_size)\n",
    "        self.cost = cost = tf.div(tf.reduce_sum(loss), batch_size, name=\"cost\")\n",
    "        \n",
    "        if is_training:\n",
    "            # set up learning rate variable to decay\n",
    "            self.lr = tf.Variable(0.98, trainable=False)\n",
    "            #self.lr = tf.Variable(0.0001, trainable=False)\n",
    "            \n",
    "            # define training operation and clip the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate=self.lr)\n",
    "            \n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train\")\n",
    "        else:\n",
    "            # if this model isn't for training (i.e. testing/validation) then we don't do anything here\n",
    "            self.train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(sess, model, data, verbose=False):\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # accumulated counts\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    \n",
    "    # initial RNN state\n",
    "    # print(model.initial_state,type(model.initial_state))\n",
    "    # state = model.initial_state.eval()\n",
    "    state = tf.get_default_session().run(model.initial_state)\n",
    "    \n",
    "    # example of how to change:\n",
    "    # state = cell.zero_state(batchsize, tf.float32).eval()\n",
    "    # state = tf.get_default_session().run(cell.zero_state(batchsize, tf.float32))\n",
    "    \n",
    "    for step, (x,y) in enumerate(ptb_reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed_dict={\n",
    "            model.input_data: x,\n",
    "            model.targets: y,\n",
    "            model.initial_state: state\n",
    "        })\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "        \n",
    "        perplexity = np.exp(costs / iters)\n",
    "        \n",
    "        if verbose and step %10 == 0:\n",
    "            progress = (step / epoch_size) * 100\n",
    "            wps = iters * model.batch_size / (time.time() - start_time)\n",
    "            print(\"Progress:  %.1f%% Perpexity: %.2f (Cost: %.2f) Speed: %.0f wps\" % (progress, perplexity, cost, wps))\n",
    "            \n",
    "        return (costs / iters), perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    batch_size = 20\n",
    "    num_steps = 35 # number of unrolled time steps\n",
    "    hidden_size = 450 # number of blocks in an LSTM cell\n",
    "    vocab_size = 10000\n",
    "    max_grad_norm = 5 # maximum gradient for clipping\n",
    "    init_scale = 0.05 # scale between -0.1 and 0.1 for all random initialization\n",
    "    keep_prob = 0.5 # dropout probability\n",
    "    num_layers = 2 # number of LSTM layers\n",
    "    #learning_rate = 0.0001 # 1.0\n",
    "    learning_rate = 0.001 # not getting used at the moment\n",
    "    lr_decay = 0.985\n",
    "    lr_decay_epoch_offset = 6 # don't decay until after the Nth epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default settings for training\n",
    "train_config = Config()\n",
    "\n",
    "# our evaluation runs (validation and testing), use a batch size and time_step of one\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "# number of epochs to perform over the training data\n",
    "num_epochs = 39\n",
    "\n",
    "import vanilla, cifg, MILSTM, mLSTM\n",
    "\n",
    "cell_types = {\n",
    "    'vanilla': vanilla.VanillaLSTMCell,\n",
    "    'cifg': cifg.CIFGLSTMCell,\n",
    "    'milstm': MILSTM.MILSTMCell,\n",
    "    'mlstm': mLSTM.mLSTMCell,\n",
    "#     'nig': NIGLSTMCell,\n",
    "#     'nfg': NFGLSTMCell,\n",
    "#     'nog': NOGLSTMCell,\n",
    "#     'niaf': NIAFLSTMCell,\n",
    "#     'noaf': NOAFLSTMCell,\n",
    "#     'np': NPLSTMCell,\n",
    "#     'fgr': FGRLSTMCell,\n",
    "}\n",
    "\n",
    "#model_name = \"vanilla\"\n",
    "#model_name = \"cifg\"\n",
    "#model_name = \"milstm\"\n",
    "model_name = \"mlstm\"\n",
    "\n",
    "CellType = cell_types[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 9997.86 (Cost: 322.35) Speed: 147 wps\n",
      "Epoch: 1 Training Perplexity: 9997.86 (Cost: 9.21)\n",
      "Epoch: 1 Validation Perplexity: 5314.70 (Cost: 8.58)\n",
      "Epoch: 2 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 5027.10 (Cost: 298.29) Speed: 266 wps\n",
      "Epoch: 2 Training Perplexity: 5027.10 (Cost: 8.52)\n",
      "Epoch: 2 Validation Perplexity: 25435.87 (Cost: 10.14)\n",
      "Epoch: 3 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 20540.49 (Cost: 347.56) Speed: 288 wps\n",
      "Epoch: 3 Training Perplexity: 20540.49 (Cost: 9.93)\n",
      "Epoch: 3 Validation Perplexity: 3440.58 (Cost: 8.14)\n",
      "Epoch: 4 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 3444.27 (Cost: 285.06) Speed: 242 wps\n",
      "Epoch: 4 Training Perplexity: 3444.27 (Cost: 8.14)\n",
      "Epoch: 4 Validation Perplexity: 4637.32 (Cost: 8.44)\n",
      "Epoch: 5 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 4147.31 (Cost: 291.56) Speed: 278 wps\n",
      "Epoch: 5 Training Perplexity: 4147.31 (Cost: 8.33)\n",
      "Epoch: 5 Validation Perplexity: 2833.91 (Cost: 7.95)\n",
      "Epoch: 6 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 2334.27 (Cost: 271.44) Speed: 215 wps\n",
      "Epoch: 6 Training Perplexity: 2334.27 (Cost: 7.76)\n",
      "Epoch: 6 Validation Perplexity: 7652.00 (Cost: 8.94)\n",
      "Epoch: 7 Learning Rate: 0.98000\n",
      "Progress:  0.0% Perpexity: 5209.39 (Cost: 299.54) Speed: 264 wps\n",
      "Epoch: 7 Training Perplexity: 5209.39 (Cost: 8.56)\n",
      "Epoch: 7 Validation Perplexity: 2160.47 (Cost: 7.68)\n",
      "Epoch: 8 Learning Rate: 0.96530\n",
      "Progress:  0.0% Perpexity: 1892.45 (Cost: 264.10) Speed: 259 wps\n",
      "Epoch: 8 Training Perplexity: 1892.45 (Cost: 7.55)\n",
      "Epoch: 8 Validation Perplexity: 2166.31 (Cost: 7.68)\n",
      "Test Perplexity: 468.42 (Cost: 6.15)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # define our training model\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        train_model = PTBModel(CellType, is_training=True, config=train_config)\n",
    "        \n",
    "    # we create a separate model for validation and testing to alter the batch_size and time steps\n",
    "    # reuse = True reuses variables from the previously defined 'train_model'\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        valid_model = PTBModel(CellType, is_training=False, config=train_config)\n",
    "        test_model = PTBModel(CellType, is_training=False, config=eval_config)\n",
    "\n",
    "    # create a saver instance to restore from the checkpoint\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    # initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # save the graph definition as a protobuf file\n",
    "    tf.train.write_graph(sess.graph_def, model_path, '%s.pb'.format(model_name), as_text=False)\n",
    "\n",
    "    train_costs = []\n",
    "    train_perps = []\n",
    "    valid_costs = []\n",
    "    valid_perps = []\n",
    "    \n",
    "    # set initial learning rate - can I do this here? so far throws an error\n",
    "    #train_model.lr = train_config.learning_rate\n",
    "    #train_model.lr = tf.Variable(train_config.learning_rate, trainable=False)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        # decay learning rate\n",
    "        if i > train_config.lr_decay_epoch_offset:\n",
    "            train_model.lr = train_model.lr * train_config.lr_decay\n",
    "        \n",
    "        print(\"Epoch: %d Learning Rate: %.5f\" % (i + 1, sess.run(train_model.lr)))\n",
    "\n",
    "        # run training pass\n",
    "        train_cost, train_perp = run_epoch(sess, train_model, train_data, verbose=True)\n",
    "        print(\"Epoch: %i Training Perplexity: %.2f (Cost: %.2f)\" % (i + 1,  train_perp, train_cost))\n",
    "        train_costs.append(train_cost)\n",
    "        train_perps.append(train_perp)\n",
    "\n",
    "        # run validation pass\n",
    "        valid_cost, valid_perp = run_epoch(sess, valid_model, valid_data)\n",
    "        print(\"Epoch: %i Validation Perplexity: %.2f (Cost: %.2f)\" % (i + 1, valid_perp, valid_cost))\n",
    "        valid_costs.append(valid_cost)\n",
    "        valid_perps.append(valid_perp)\n",
    "\n",
    "        # save(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\n",
    "        saver.save(sess, checkpoint_path + 'checkpoint', global_step=i,latest_filename=None)\n",
    "\n",
    "    # run test pass\n",
    "    test_cost, test_perp = run_epoch(sess, test_model, test_data)\n",
    "    print(\"Test Perplexity: %.2f (Cost: %.2f)\" % (test_perp, test_cost))\n",
    "\n",
    "    write_csv(train_costs, os.path.join(summary_path, \"train_costs.csv\"))\n",
    "    write_csv(train_perps, os.path.join(summary_path, \"train_perps.csv\"))\n",
    "    write_csv(valid_costs, os.path.join(summary_path, \"valid_costs.csv\"))\n",
    "    write_csv(valid_perps, os.path.join(summary_path, \"valid_perps.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
