{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.models.rnn.seq2seq import sequence_loss_by_example\n",
    "\n",
    "from DropoutWrapper import DropoutWrapper\n",
    "from MultiRNNCell2 import MultiRNNCell2\n",
    "\n",
    "# parses the dataset\n",
    "import ptb_reader\n",
    "\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/1A_Tensorflow/AA_RNN_Practice/TF_RNN_1/my_lstm_odyssey\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define artifact directories where results from the session can be saved\n",
    "model_path = os.environ.get('MODEL_PATH', 'models/')\n",
    "checkpoint_path = os.environ.get('CHECKPOINT_PATH', 'checkpoints/')\n",
    "summary_path = os.environ.get('SUMMARY_PATH', 'logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/1A_Tensorflow/AA_RNN_Practice/TF_RNN_1/my_lstm_odyssey\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_data, valid_data, test_data, _ = ptb_reader.ptb_raw_data(data_path=\"ptb\")\n",
    "\n",
    "def write_csv(arr, path):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Adapted to support sampled_softmax loss function, which accepts activations\n",
    "# instead of logits.\n",
    "def sequence_loss_by_example(inputs, targets, weights, loss_function,\n",
    "                             average_across_timesteps=True, name=None):\n",
    "  \"\"\"Sampled softmax loss for a sequence of inputs (per example).\n",
    "  Args:\n",
    "    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\n",
    "    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "    loss_function: Sampled softmax function (inputs, labels) -> loss\n",
    "    average_across_timesteps: If set, divide the returned cost by the total\n",
    "      label weight.\n",
    "    name: Optional name for this operation, default: 'sequence_loss_by_example'.\n",
    "  Returns:\n",
    "    1D batch-sized float Tensor: The log-perplexity for each sequence.\n",
    "  Raises:\n",
    "    ValueError: If len(inputs) is different from len(targets) or len(weights).\n",
    "  \"\"\"\n",
    "  if len(targets) != len(inputs) or len(weights) != len(inputs):\n",
    "    raise ValueError('Lengths of logits, weights, and targets must be the same '\n",
    "                     '%d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n",
    "  with tf.name_scope(values=inputs + targets + weights, name=name,\n",
    "                     default_name='sequence_loss_by_example'):\n",
    "    log_perp_list = []\n",
    "    for inp, target, weight in zip(inputs, targets, weights):\n",
    "      crossent = loss_function(inp, target)\n",
    "      log_perp_list.append(crossent * weight)\n",
    "    log_perps = tf.add_n(log_perp_list)\n",
    "    if average_across_timesteps:\n",
    "      total_size = tf.add_n(weights)\n",
    "      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "      log_perps /= total_size\n",
    "  return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.12.1 version\n",
    "def sequence_loss_by_example(logits, targets, weights,\n",
    "                             average_across_timesteps=True,\n",
    "                             softmax_loss_function=None, name=None):\n",
    "  \"\"\"Weighted cross-entropy loss for a sequence of logits (per example).\n",
    "\n",
    "  Args:\n",
    "    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n",
    "    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "    average_across_timesteps: If set, divide the returned cost by the total\n",
    "      label weight.\n",
    "    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n",
    "      to be used instead of the standard softmax (the default if this is None).\n",
    "    name: Optional name for this operation, default: \"sequence_loss_by_example\".\n",
    "\n",
    "  Returns:\n",
    "    1D batch-sized float Tensor: The log-perplexity for each sequence.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If len(logits) is different from len(targets) or len(weights).\n",
    "  \"\"\"\n",
    "  if len(targets) != len(logits) or len(weights) != len(logits):\n",
    "    raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n",
    "                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n",
    "  with tf.name_scope(name, \"sequence_loss_by_example\",\n",
    "                      logits + targets + weights):\n",
    "    log_perp_list = []\n",
    "    for logit, target, weight in zip(logits, targets, weights):\n",
    "      if softmax_loss_function is None:\n",
    "        # TODO(irving,ebrevdo): This reshape is needed because\n",
    "        # sequence_loss_by_example is called with scalars sometimes, which\n",
    "        # violates our general scalar strictness policy.\n",
    "        target = tf.reshape(target, [-1])\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logit, labels=target)\n",
    "      else:\n",
    "        crossent = softmax_loss_function(logit, target)\n",
    "      log_perp_list.append(crossent * weight)\n",
    "    log_perps = tf.add_n(log_perp_list)\n",
    "    if average_across_timesteps:\n",
    "      total_size = tf.add_n(weights)\n",
    "      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "      log_perps /= total_size\n",
    "  return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tf.models.rnn.seq2seq import sequence_loss_by_example\n",
    "\n",
    "class PTBModel(object):\n",
    "    def __init__(self, CellType, is_training, config):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "        \n",
    "        lstm_cell = CellType(size)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob = config.keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # initalizer used for reusable variable initializer (see 'get_variable')\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "        \n",
    "#        with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, size], initializer=initializer)\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "            \n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "            \n",
    "        outputs = []\n",
    "        states = []\n",
    "        state = self.initial_state\n",
    "        \n",
    "        with tf.variable_scope(\"RNN\", initializer=initializer):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    \n",
    "                inputs_slice = inputs[:,time_step,:]\n",
    "                (cell_output, state) = cell(inputs_slice, state)\n",
    "                \n",
    "                outputs.append(cell_output)\n",
    "                states.append(state)\n",
    "                \n",
    "        self.final_state = states[-1]\n",
    "        \n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, size])\n",
    "        w = tf.get_variable(\"softmax_w\",\n",
    "                           [size, vocab_size],\n",
    "                           initializer=initializer)\n",
    "        b = tf.get_variable(\"softmax_b\", [vocab_size], initializer=initializer)\n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(output, w, b) # compute logits for loss\n",
    "        targets = tf.reshape(self.targets, [-1]) # reshape target outputs\n",
    "        weights = tf.ones([batch_size * num_steps]) # used to scale the loss average\n",
    "        \n",
    "        # computes loss and performs softmax on our fully-connected output layer\n",
    "#         def sequence_loss_by_example(inputs, targets, weights, loss_function,\n",
    "#                              average_across_timesteps=True, name=None):\n",
    "        \n",
    "        loss = sequence_loss_by_example([logits], [targets], [weights], vocab_size)\n",
    "        self.cost = cost = tf.div(tf.reduce_sum(loss), batch_size, name=\"cost\")\n",
    "        \n",
    "        if is_training:\n",
    "            # set up learning rate variable to decay\n",
    "            self.lr = tf.Variable(0.98, trainable=False)\n",
    "            #self.lr = tf.Variable(0.0001, trainable=False)\n",
    "            \n",
    "            # define training operation and clip the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate=self.lr)\n",
    "            \n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), name=\"train\")\n",
    "        else:\n",
    "            # if this model isn't for training (i.e. testing/validation) then we don't do anything here\n",
    "            self.train_op = tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(sess, model, data, verbose=False):\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # accumulated counts\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    \n",
    "    # initial RNN state\n",
    "    # print(model.initial_state,type(model.initial_state))\n",
    "    # state = model.initial_state.eval()\n",
    "    state = tf.get_default_session().run(model.initial_state)\n",
    "    \n",
    "    # example of how to change:\n",
    "    # state = cell.zero_state(batchsize, tf.float32).eval()\n",
    "    # state = tf.get_default_session().run(cell.zero_state(batchsize, tf.float32))\n",
    "    \n",
    "    for step, (x,y) in enumerate(ptb_reader.ptb_iterator(data, model.batch_size, model.num_steps)):\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed_dict={\n",
    "            model.input_data: x,\n",
    "            model.targets: y,\n",
    "            model.initial_state: state\n",
    "        })\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "        \n",
    "        perplexity = np.exp(costs / iters)\n",
    "        \n",
    "        if verbose and step %10 == 0:\n",
    "            progress = (step / epoch_size) * 100\n",
    "            wps = iters * model.batch_size / (time.time() - start_time)\n",
    "            print(\"Progress:  %.1f%% Perpexity: %.2f (Cost: %.2f) Speed: %.0f wps\" % (progress, perplexity, cost, wps))\n",
    "            \n",
    "        return (costs / iters), perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    batch_size = 20\n",
    "    num_steps = 35 # number of unrolled time steps\n",
    "    hidden_size = 450 # number of blocks in an LSTM cell\n",
    "    vocab_size = 10000\n",
    "    max_grad_norm = 5 # maximum gradient for clipping\n",
    "    init_scale = 0.05 # scale between -0.1 and 0.1 for all random initialization\n",
    "    keep_prob = 0.5 # dropout probability\n",
    "    num_layers = 2 # number of LSTM layers\n",
    "    #learning_rate = 0.0001 # 1.0\n",
    "    learning_rate = 0.001 # not getting used at the moment\n",
    "    lr_decay = 0.985\n",
    "    lr_decay_epoch_offset = 6 # don't decay until after the Nth epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default settings for training\n",
    "train_config = Config()\n",
    "\n",
    "# our evaluation runs (validation and testing), use a batch size and time_step of one\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "# number of epochs to perform over the training data\n",
    "num_epochs = 39\n",
    "\n",
    "import vanilla, cifg\n",
    "\n",
    "cell_types = {\n",
    "    'vanilla': vanilla.VanillaLSTMCell,\n",
    "#     'nig': NIGLSTMCell,\n",
    "#     'nfg': NFGLSTMCell,\n",
    "#     'nog': NOGLSTMCell,\n",
    "#     'niaf': NIAFLSTMCell,\n",
    "#     'noaf': NOAFLSTMCell,\n",
    "#     'np': NPLSTMCell,\n",
    "     'cifg': cifg.CIFGLSTMCell,\n",
    "#     'fgr': FGRLSTMCell,\n",
    "}\n",
    "\n",
    "#model_name = \"vanilla\"\n",
    "model_name = \"cifg\"\n",
    "\n",
    "CellType = cell_types[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 10045.00 (Cost: 322.52) Speed: 214 wps\n",
      "Epoch: 1 Training Perplexity: 10045.00 (Cost: 9.21)\n",
      "Epoch: 1 Validation Perplexity: 5833.05 (Cost: 8.67)\n",
      "Epoch: 2 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 5584.91 (Cost: 301.97) Speed: 331 wps\n",
      "Epoch: 2 Training Perplexity: 5584.91 (Cost: 8.63)\n",
      "Epoch: 2 Validation Perplexity: 20735.91 (Cost: 9.94)\n",
      "Epoch: 3 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 16301.92 (Cost: 339.47) Speed: 365 wps\n",
      "Epoch: 3 Training Perplexity: 16301.92 (Cost: 9.70)\n",
      "Epoch: 3 Validation Perplexity: 5898.30 (Cost: 8.68)\n",
      "Epoch: 4 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 6351.15 (Cost: 306.47) Speed: 370 wps\n",
      "Epoch: 4 Training Perplexity: 6351.15 (Cost: 8.76)\n",
      "Epoch: 4 Validation Perplexity: 3753.97 (Cost: 8.23)\n",
      "Epoch: 5 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 3640.75 (Cost: 287.00) Speed: 359 wps\n",
      "Epoch: 5 Training Perplexity: 3640.75 (Cost: 8.20)\n",
      "Epoch: 5 Validation Perplexity: 8123.57 (Cost: 9.00)\n",
      "Epoch: 6 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 7831.39 (Cost: 313.81) Speed: 361 wps\n",
      "Epoch: 6 Training Perplexity: 7831.39 (Cost: 8.97)\n",
      "Epoch: 6 Validation Perplexity: 3220.61 (Cost: 8.08)\n",
      "Epoch: 7 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 2930.28 (Cost: 279.40) Speed: 351 wps\n",
      "Epoch: 7 Training Perplexity: 2930.28 (Cost: 7.98)\n",
      "Epoch: 7 Validation Perplexity: 16954.49 (Cost: 9.74)\n",
      "Epoch: 8 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 13693.65 (Cost: 333.36) Speed: 340 wps\n",
      "Epoch: 8 Training Perplexity: 13693.65 (Cost: 9.52)\n",
      "Epoch: 8 Validation Perplexity: 4855.01 (Cost: 8.49)\n",
      "Epoch: 9 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 5494.89 (Cost: 301.41) Speed: 319 wps\n",
      "Epoch: 9 Training Perplexity: 5494.89 (Cost: 8.61)\n",
      "Epoch: 9 Validation Perplexity: 3750.27 (Cost: 8.23)\n",
      "Epoch: 10 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 3320.42 (Cost: 283.77) Speed: 363 wps\n",
      "Epoch: 10 Training Perplexity: 3320.42 (Cost: 8.11)\n",
      "Epoch: 10 Validation Perplexity: 5966.29 (Cost: 8.69)\n",
      "Epoch: 11 Learning Rate: 0.98000\n",
      "0.0% Perpexity: 4737.27 (Cost: 296.21) Speed: 301 wps\n",
      "Epoch: 11 Training Perplexity: 4737.27 (Cost: 8.46)\n",
      "Epoch: 11 Validation Perplexity: 3157.21 (Cost: 8.06)\n",
      "Epoch: 12 Learning Rate: 0.97020\n",
      "0.0% Perpexity: 2948.64 (Cost: 279.62) Speed: 296 wps\n",
      "Epoch: 12 Training Perplexity: 2948.64 (Cost: 7.99)\n",
      "Epoch: 12 Validation Perplexity: 3585.80 (Cost: 8.18)\n",
      "Epoch: 13 Learning Rate: 0.96050\n",
      "0.0% Perpexity: 3007.16 (Cost: 280.31) Speed: 278 wps\n",
      "Epoch: 13 Training Perplexity: 3007.16 (Cost: 8.01)\n",
      "Epoch: 13 Validation Perplexity: 1877.51 (Cost: 7.54)\n",
      "Epoch: 14 Learning Rate: 0.95089\n",
      "0.0% Perpexity: 1372.36 (Cost: 252.85) Speed: 351 wps\n",
      "Epoch: 14 Training Perplexity: 1372.36 (Cost: 7.22)\n",
      "Epoch: 14 Validation Perplexity: 6073.55 (Cost: 8.71)\n",
      "Epoch: 15 Learning Rate: 0.94138\n",
      "0.0% Perpexity: 3745.45 (Cost: 287.99) Speed: 317 wps\n",
      "Epoch: 15 Training Perplexity: 3745.45 (Cost: 8.23)\n",
      "Epoch: 15 Validation Perplexity: 2741.53 (Cost: 7.92)\n",
      "Epoch: 16 Learning Rate: 0.93197\n",
      "0.0% Perpexity: 1635.39 (Cost: 258.99) Speed: 303 wps\n",
      "Epoch: 16 Training Perplexity: 1635.39 (Cost: 7.40)\n",
      "Epoch: 16 Validation Perplexity: 2813.26 (Cost: 7.94)\n",
      "Epoch: 17 Learning Rate: 0.92265\n",
      "0.0% Perpexity: 1973.13 (Cost: 265.56) Speed: 360 wps\n",
      "Epoch: 17 Training Perplexity: 1973.13 (Cost: 7.59)\n",
      "Epoch: 17 Validation Perplexity: 1973.19 (Cost: 7.59)\n",
      "Epoch: 18 Learning Rate: 0.91342\n",
      "0.0% Perpexity: 1241.01 (Cost: 249.33) Speed: 308 wps\n",
      "Epoch: 18 Training Perplexity: 1241.01 (Cost: 7.12)\n",
      "Epoch: 18 Validation Perplexity: 2506.51 (Cost: 7.83)\n",
      "Epoch: 19 Learning Rate: 0.90429\n",
      "0.0% Perpexity: 1286.73 (Cost: 250.60) Speed: 324 wps\n",
      "Epoch: 19 Training Perplexity: 1286.73 (Cost: 7.16)\n",
      "Epoch: 19 Validation Perplexity: 2465.78 (Cost: 7.81)\n",
      "Epoch: 20 Learning Rate: 0.89525\n",
      "0.0% Perpexity: 1390.04 (Cost: 253.30) Speed: 338 wps\n",
      "Epoch: 20 Training Perplexity: 1390.04 (Cost: 7.24)\n",
      "Epoch: 20 Validation Perplexity: 2265.31 (Cost: 7.73)\n",
      "Epoch: 21 Learning Rate: 0.88629\n",
      "0.0% Perpexity: 1179.39 (Cost: 247.55) Speed: 340 wps\n",
      "Epoch: 21 Training Perplexity: 1179.39 (Cost: 7.07)\n",
      "Epoch: 21 Validation Perplexity: 27069.78 (Cost: 10.21)\n",
      "Epoch: 22 Learning Rate: 0.87743\n",
      "0.0% Perpexity: 10199.77 (Cost: 323.05) Speed: 356 wps\n",
      "Epoch: 22 Training Perplexity: 10199.77 (Cost: 9.23)\n",
      "Epoch: 22 Validation Perplexity: 2027.34 (Cost: 7.61)\n",
      "Epoch: 23 Learning Rate: 0.86866\n",
      "0.0% Perpexity: 1080.49 (Cost: 244.48) Speed: 330 wps\n",
      "Epoch: 23 Training Perplexity: 1080.49 (Cost: 6.99)\n",
      "Epoch: 23 Validation Perplexity: 2099.79 (Cost: 7.65)\n",
      "Epoch: 24 Learning Rate: 0.85997\n",
      "0.0% Perpexity: 976.74 (Cost: 240.95) Speed: 366 wps\n",
      "Epoch: 24 Training Perplexity: 976.74 (Cost: 6.88)\n",
      "Epoch: 24 Validation Perplexity: 1991.70 (Cost: 7.60)\n",
      "Epoch: 25 Learning Rate: 0.85137\n",
      "0.0% Perpexity: 938.30 (Cost: 239.54) Speed: 338 wps\n",
      "Epoch: 25 Training Perplexity: 938.30 (Cost: 6.84)\n",
      "Epoch: 25 Validation Perplexity: 2172.92 (Cost: 7.68)\n",
      "Epoch: 26 Learning Rate: 0.84286\n",
      "0.0% Perpexity: 954.19 (Cost: 240.13) Speed: 353 wps\n",
      "Epoch: 26 Training Perplexity: 954.19 (Cost: 6.86)\n",
      "Epoch: 26 Validation Perplexity: 1796.97 (Cost: 7.49)\n",
      "Epoch: 27 Learning Rate: 0.83443\n",
      "0.0% Perpexity: 683.40 (Cost: 228.45) Speed: 319 wps\n",
      "Epoch: 27 Training Perplexity: 683.40 (Cost: 6.53)\n",
      "Epoch: 27 Validation Perplexity: 2554.49 (Cost: 7.85)\n",
      "Epoch: 28 Learning Rate: 0.82608\n",
      "0.0% Perpexity: 850.88 (Cost: 236.12) Speed: 313 wps\n",
      "Epoch: 28 Training Perplexity: 850.88 (Cost: 6.75)\n",
      "Epoch: 28 Validation Perplexity: 2794.48 (Cost: 7.94)\n",
      "Epoch: 29 Learning Rate: 0.81782\n",
      "0.0% Perpexity: 1033.64 (Cost: 242.93) Speed: 324 wps\n",
      "Epoch: 29 Training Perplexity: 1033.64 (Cost: 6.94)\n",
      "Epoch: 29 Validation Perplexity: 2474.31 (Cost: 7.81)\n",
      "Epoch: 30 Learning Rate: 0.80965\n",
      "0.0% Perpexity: 960.34 (Cost: 240.35) Speed: 313 wps\n",
      "Epoch: 30 Training Perplexity: 960.34 (Cost: 6.87)\n",
      "Epoch: 30 Validation Perplexity: 4372.53 (Cost: 8.38)\n",
      "Epoch: 31 Learning Rate: 0.80155\n",
      "0.0% Perpexity: 1338.39 (Cost: 251.97) Speed: 345 wps\n",
      "Epoch: 31 Training Perplexity: 1338.39 (Cost: 7.20)\n",
      "Epoch: 31 Validation Perplexity: 2086.71 (Cost: 7.64)\n",
      "Epoch: 32 Learning Rate: 0.79353\n",
      "0.0% Perpexity: 816.41 (Cost: 234.67) Speed: 394 wps\n",
      "Epoch: 32 Training Perplexity: 816.41 (Cost: 6.70)\n",
      "Epoch: 32 Validation Perplexity: 1821.85 (Cost: 7.51)\n",
      "Epoch: 33 Learning Rate: 0.78560\n",
      "0.0% Perpexity: 578.50 (Cost: 222.62) Speed: 361 wps\n",
      "Epoch: 33 Training Perplexity: 578.50 (Cost: 6.36)\n",
      "Epoch: 33 Validation Perplexity: 2210.21 (Cost: 7.70)\n",
      "Epoch: 34 Learning Rate: 0.77774\n",
      "0.0% Perpexity: 654.59 (Cost: 226.94) Speed: 379 wps\n",
      "Epoch: 34 Training Perplexity: 654.59 (Cost: 6.48)\n",
      "Epoch: 34 Validation Perplexity: 2392.56 (Cost: 7.78)\n",
      "Epoch: 35 Learning Rate: 0.76996\n",
      "0.0% Perpexity: 798.28 (Cost: 233.89) Speed: 380 wps\n",
      "Epoch: 35 Training Perplexity: 798.28 (Cost: 6.68)\n",
      "Epoch: 35 Validation Perplexity: 1706.14 (Cost: 7.44)\n",
      "Epoch: 36 Learning Rate: 0.76227\n",
      "0.0% Perpexity: 469.93 (Cost: 215.34) Speed: 309 wps\n",
      "Epoch: 36 Training Perplexity: 469.93 (Cost: 6.15)\n",
      "Epoch: 36 Validation Perplexity: 54714.03 (Cost: 10.91)\n",
      "Epoch: 37 Learning Rate: 0.75464\n",
      "0.0% Perpexity: 10073.86 (Cost: 322.62) Speed: 367 wps\n",
      "Epoch: 37 Training Perplexity: 10073.86 (Cost: 9.22)\n",
      "Epoch: 37 Validation Perplexity: 2467.48 (Cost: 7.81)\n",
      "Epoch: 38 Learning Rate: 0.74710\n",
      "0.0% Perpexity: 708.90 (Cost: 229.73) Speed: 325 wps\n",
      "Epoch: 38 Training Perplexity: 708.90 (Cost: 6.56)\n",
      "Epoch: 38 Validation Perplexity: 2380.80 (Cost: 7.78)\n",
      "Epoch: 39 Learning Rate: 0.73963\n",
      "0.0% Perpexity: 619.26 (Cost: 225.00) Speed: 319 wps\n",
      "Epoch: 39 Training Perplexity: 619.26 (Cost: 6.43)\n",
      "Epoch: 39 Validation Perplexity: 10252.57 (Cost: 9.24)\n",
      "Test Perplexity: 628.68 (Cost: 6.44)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # define our training model\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        train_model = PTBModel(CellType, is_training=True, config=train_config)\n",
    "        \n",
    "    # we create a separate model for validation and testing to alter the batch_size and time steps\n",
    "    # reuse = True reuses variables from the previously defined 'train_model'\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        valid_model = PTBModel(CellType, is_training=False, config=train_config)\n",
    "        test_model = PTBModel(CellType, is_training=False, config=eval_config)\n",
    "\n",
    "    # create a saver instance to restore from the checkpoint\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    # initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # save the graph definition as a protobuf file\n",
    "    tf.train.write_graph(sess.graph_def, model_path, '%s.pb'.format(model_name), as_text=False)\n",
    "\n",
    "    train_costs = []\n",
    "    train_perps = []\n",
    "    valid_costs = []\n",
    "    valid_perps = []\n",
    "    \n",
    "    # set initial learning rate - can I do this here? so far throws an error\n",
    "    #train_model.lr = train_config.learning_rate\n",
    "    #train_model.lr = tf.Variable(train_config.learning_rate, trainable=False)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        # decay learning rate\n",
    "        if i > train_config.lr_decay_epoch_offset:\n",
    "            train_model.lr = train_model.lr * train_config.lr_decay\n",
    "        \n",
    "        print(\"Epoch: %d Learning Rate: %.5f\" % (i + 1, sess.run(train_model.lr)))\n",
    "\n",
    "        # run training pass\n",
    "        train_cost, train_perp = run_epoch(sess, train_model, train_data, verbose=True)\n",
    "        print(\"Epoch: %i Training Perplexity: %.2f (Cost: %.2f)\" % (i + 1,  train_perp, train_cost))\n",
    "        train_costs.append(train_cost)\n",
    "        train_perps.append(train_perp)\n",
    "\n",
    "        # run validation pass\n",
    "        valid_cost, valid_perp = run_epoch(sess, valid_model, valid_data)\n",
    "        print(\"Epoch: %i Validation Perplexity: %.2f (Cost: %.2f)\" % (i + 1, valid_perp, valid_cost))\n",
    "        valid_costs.append(valid_cost)\n",
    "        valid_perps.append(valid_perp)\n",
    "\n",
    "        # save(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\n",
    "        saver.save(sess, checkpoint_path + 'checkpoint', global_step=i,latest_filename=None)\n",
    "\n",
    "    # run test pass\n",
    "    test_cost, test_perp = run_epoch(sess, test_model, test_data)\n",
    "    print(\"Test Perplexity: %.2f (Cost: %.2f)\" % (test_perp, test_cost))\n",
    "\n",
    "    write_csv(train_costs, os.path.join(summary_path, \"train_costs.csv\"))\n",
    "    write_csv(train_perps, os.path.join(summary_path, \"train_perps.csv\"))\n",
    "    write_csv(valid_costs, os.path.join(summary_path, \"valid_costs.csv\"))\n",
    "    write_csv(valid_perps, os.path.join(summary_path, \"valid_perps.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
