{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from integrated import BasicRNNCell, MultiRNNCell, dynamic_rnn\n",
    "%autosave 0\n",
    "\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import nn_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _linear(args, output_size, bias, bias_start=0.0):\n",
    "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "  Args:\n",
    "    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    bias: boolean, whether to add a bias term or not.\n",
    "    bias_start: starting value to initialize the bias; 0 by default.\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  \"\"\"\n",
    "  if args is None or (nest.is_sequence(args) and not args):\n",
    "    raise ValueError(\"`args` must be specified\")\n",
    "  if not nest.is_sequence(args):\n",
    "    args = [args]\n",
    "\n",
    "  # Calculate the total size of arguments on dimension 1.\n",
    "  total_arg_size = 0\n",
    "  shapes = [a.get_shape() for a in args]\n",
    "  for shape in shapes:\n",
    "    if shape.ndims != 2:\n",
    "      raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n",
    "    if shape[1].value is None:\n",
    "      raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n",
    "                       \"but saw %s\" % (shape, shape[1]))\n",
    "    else:\n",
    "      total_arg_size += shape[1].value\n",
    "\n",
    "  dtype = [a.dtype for a in args][0]\n",
    "\n",
    "  # Now the computation.\n",
    "  scope = vs.get_variable_scope()\n",
    "  with vs.variable_scope(scope) as outer_scope:\n",
    "    weights = vs.get_variable(\n",
    "        \"weights\", [total_arg_size, output_size], dtype=dtype)\n",
    "    \n",
    "    # either one argument or more\n",
    "    if len(args) == 1:\n",
    "      res = math_ops.matmul(args[0], weights)\n",
    "    else:\n",
    "      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n",
    "    \n",
    "    # add biases if they exist\n",
    "    if not bias:\n",
    "      return res\n",
    "    with vs.variable_scope(outer_scope) as inner_scope:\n",
    "      inner_scope.set_partitioner(None)\n",
    "      biases = vs.get_variable(\n",
    "          \"biases\", [output_size],\n",
    "          dtype=dtype,\n",
    "          initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\n",
    "        \n",
    "    return nn_ops.bias_add(res, biases)\n",
    "\n",
    "class RNNCell2(object):\n",
    "  \"\"\"\n",
    "  This definition of cell differs from the definition used in the literature.\n",
    "  In the literature, 'cell' refers to an object with a single scalar output.\n",
    "  This definition refers to a horizontal array of such units.\n",
    "  An RNN cell, in the most abstract setting, is anything that has\n",
    "  a state and performs some operation that takes a matrix of inputs.\n",
    "  This operation results in an output matrix with `self.output_size` columns.\n",
    "  If `self.state_size` is an integer, this operation also results in a new\n",
    "  state matrix with `self.state_size` columns.  If `self.state_size` is a\n",
    "  tuple of integers, then it results in a tuple of `len(state_size)` state\n",
    "  matrices, each with a column size corresponding to values in `state_size`.\n",
    "  \"\"\"\n",
    "  def __init__(self, num_units, activation=tf.tanh, reuse=None):\n",
    "    self._num_units = num_units\n",
    "    self._activation = activation\n",
    "    self._reuse = reuse\n",
    "    \n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run this RNN cell on inputs, starting from the given state.\n",
    "    Args:\n",
    "      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "      state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n",
    "        with shape `[batch_size x self.state_size]`.  Otherwise, if\n",
    "        `self.state_size` is a tuple of integers, this should be a tuple\n",
    "        with shapes `[batch_size x s] for s in self.state_size`.\n",
    "      scope: VariableScope for the created subgraph; defaults to class name.\n",
    "    Returns:\n",
    "      A pair containing:\n",
    "      - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n",
    "      - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
    "        the arity and shapes of `state`.\n",
    "    \"\"\"\n",
    "    \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "    \"\"\" U is W.hh in karpathy's code ???\"\"\"\n",
    "    output = self._activation(_linear([inputs, state], self._num_units, True))\n",
    "    return output, output\n",
    "\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    \"\"\"size(s) of state(s) used by this cell.\n",
    "    It can be represented by an Integer, a TensorShape or a tuple of Integers\n",
    "    or TensorShapes.\n",
    "    \"\"\"\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    \"\"\"Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n",
    "    return self._num_units\n",
    "\n",
    "  def zero_state(self, batch_size, dtype):\n",
    "    \"\"\"Return zero-filled state tensor(s).\n",
    "    Args:\n",
    "      batch_size: int, float, or unit Tensor representing the batch size.\n",
    "      dtype: the data type to use for the state.\n",
    "    Returns:\n",
    "      If `state_size` is an int or TensorShape, then the return value is a\n",
    "      `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n",
    "      If `state_size` is a nested list or tuple, then the return value is\n",
    "      a nested list or tuple (of the same structure) of `2-D` tensors with\n",
    "      the shapes `[batch_size x s]` for each s in `state_size`.\n",
    "    \"\"\"\n",
    "    with ops.name_scope(type(self).__name__ + \"ZeroState\", values=[batch_size]):\n",
    "      state_size = self.state_size\n",
    "      return _zero_state_tensors(state_size, batch_size, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(tensor,batch_size):\n",
    "    i = 0\n",
    "    while i < tensor.shape[0]//batch_size:\n",
    "        yield tensor[i:i+batch_size]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "num_units = 100\n",
    "n = 1000\n",
    "batch_size = 50\n",
    "X = np.arange(n*num_units).reshape((n,num_units))\n",
    "X_batches = X[:batch_size]\n",
    "#W = np.arange(9)[::-1].reshape((3,3))\n",
    "W = np.arange(num_units**2*2)[::-1].reshape((num_units*2,num_units))\n",
    "#h = np.ones(n*num_units).reshape((n,num_units)) # adding second dimension\n",
    "h = np.ones(batch_size*num_units).reshape((batch_size,num_units)) # adding second dimension\n",
    "B = np.arange(num_units)\n",
    "print(X)\n",
    "print()\n",
    "print(W)\n",
    "print()\n",
    "print(h)\n",
    "print()\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "weights = []\n",
    "biases = []\n",
    "\n",
    "with vs.variable_scope(scope) as outer_scope:\n",
    "    # now becomes variable type which is a pointer to something inside the graph\n",
    "    weights = tf.get_variable(\"weights\",[num_units*2,num_units],dtype=tf.float32)\n",
    "    biases = tf.get_variable(\"biases\",[num_units],dtype=tf.float32)\n",
    "\n",
    "# now that we've set reuse no more variable creation\n",
    "scope.reuse_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer()\n",
    "    \n",
    "    out = sess.run(_linear([inputs,hidden_state], num_units, bias=True),feed_dict = {\n",
    "        weights: W,\n",
    "        inputs: x,\n",
    "        biases: B,\n",
    "        hidden_state: h\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot_generator(label,X):\n",
    "    X = np.array(X)\n",
    "    for x in X:\n",
    "        this_one = np.zeros(len(X))\n",
    "        this_one[np.where(X==label)] = 1\n",
    "    return this_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "scope = vs.get_variable_scope()\n",
    "\n",
    "X = \"hello\"*3\n",
    "voc = np.unique(list(X))\n",
    "x = 'h' # I expect to get [0,1,0,0]\n",
    "# ii = np.where(voc==x)\n",
    "# z = tf.one_hot(ii[0],depth=len(voc))\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_hot_array = []\n",
    "for char in X:\n",
    "    one_hot_stuff = one_hot_generator(char,voc)\n",
    "    one_hot_array.append(one_hot_stuff)\n",
    "one_hot_array = np.array(one_hot_array)\n",
    "print(one_hot_array.shape)\n",
    "print()\n",
    "#one_hot_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen = batch_generator(X,batch_size)\n",
    "#x = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4) (4, 4) (4,) (15, 4)\n",
      "(4, 4) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "num_units = 4\n",
    "#n = 10\n",
    "batch_size = len(voc)\n",
    "\n",
    "# x = X[:batch_size] # 'h'\n",
    "# x_one_hot = one_hot_generator(x,voc) # [0,1,0,0]\n",
    "\n",
    "W = np.arange(num_units**2*2)[::-1].reshape((num_units*2,num_units))\n",
    "h = np.ones(batch_size*num_units).reshape((batch_size,num_units)) # adding second dimension\n",
    "B = np.arange(num_units)\n",
    "\n",
    "x = tf.expand_dims(one_hot_array[0],axis=1)\n",
    "x = tf.cast(x,dtype=tf.float32)\n",
    "print(W.shape,h.shape,B.shape,one_hot_array.shape)\n",
    "print(h.shape,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trying to share variable weights, but specified shape (5, 4) and found shape (8, 4).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b1cb839e46e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     out = sess.run(_linear([inputs,hidden_state], num_units, bias=True),feed_dict = {\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c63f54ec368f>\u001b[0m in \u001b[0;36m_linear\u001b[0;34m(args, output_size, bias, bias_start)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mouter_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     weights = vs.get_variable(\n\u001b[0;32m---> 37\u001b[0;31m         \"weights\", [total_arg_size, output_size], dtype=dtype)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# either one argument or more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1032\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1035\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1036\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    931\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    656\u001b[0m         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\n\u001b[1;32m    657\u001b[0m                          \" and found shape %s.\" % (name, shape,\n\u001b[0;32m--> 658\u001b[0;31m                                                    found_var.get_shape()))\n\u001b[0m\u001b[1;32m    659\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0mdtype_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to share variable weights, but specified shape (5, 4) and found shape (8, 4)."
     ]
    }
   ],
   "source": [
    "#inputs = one_hot_array\n",
    "#hidden_state = tf.Variable(h)\n",
    "inputs = tf.Variable(initial_value=x,dtype=tf.float32)\n",
    "hidden_state = tf.Variable(initial_value=h,dtype=tf.float32)\n",
    "\n",
    "with vs.variable_scope(scope) as outer_scope:\n",
    "    # now becomes variable type which is a pointer to something inside the graph\n",
    "    weights = tf.get_variable(\"weights\",[num_units*2,num_units],dtype=tf.float32)\n",
    "    biases = tf.get_variable(\"biases\",[num_units],dtype=tf.float32)\n",
    "\n",
    "# now that we've set reuse no more variable creation\n",
    "scope.reuse_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer()\n",
    "    out = sess.run(_linear([inputs,hidden_state], num_units, bias=True),feed_dict = {\n",
    "        weights: W,\n",
    "        inputs: x,\n",
    "        biases: B,\n",
    "        hidden_state: h\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    cell_base = BasicRNNCell(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
