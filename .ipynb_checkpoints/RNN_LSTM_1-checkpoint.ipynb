{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "\n",
    "from integrated import BasicRNNCell, MultiRNNCell, dynamic_rnn\n",
    "from core_rnn_cell_impl import _linear, checked_scope\n",
    "\n",
    "%autosave 0\n",
    "\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import nn_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _state_size_with_prefix(state_size, prefix=None):\n",
    "  \"\"\"Helper function that enables int or TensorShape shape specification.\n",
    "  This function takes a size specification, which can be an integer or a\n",
    "  TensorShape, and converts it into a list of integers. One may specify any\n",
    "  additional dimensions that precede the final state size specification.\n",
    "  Args:\n",
    "    state_size: TensorShape or int that specifies the size of a tensor.\n",
    "    prefix: optional additional list of dimensions to prepend.\n",
    "  Returns:\n",
    "    result_state_size: list of dimensions the resulting tensor size.\n",
    "  \"\"\"\n",
    "  result_state_size = tensor_shape.as_shape(state_size).as_list()\n",
    "  if prefix is not None:\n",
    "    if not isinstance(prefix, list):\n",
    "      raise TypeError(\"prefix of _state_size_with_prefix should be a list.\")\n",
    "    result_state_size = prefix + result_state_size\n",
    "  return result_state_size\n",
    "\n",
    "\n",
    "def _zero_state_tensors(state_size, batch_size, dtype):\n",
    "  \"\"\"Create tensors of zeros based on state_size, batch_size, and dtype.\"\"\"\n",
    "  if nest.is_sequence(state_size):\n",
    "    state_size_flat = nest.flatten(state_size)\n",
    "    zeros_flat = [\n",
    "        array_ops.zeros(\n",
    "            array_ops.stack(_state_size_with_prefix(\n",
    "                s, prefix=[batch_size])),\n",
    "            dtype=dtype) for s in state_size_flat\n",
    "    ]\n",
    "    for s, z in zip(state_size_flat, zeros_flat):\n",
    "      z.set_shape(_state_size_with_prefix(s, prefix=[None]))\n",
    "    zeros = nest.pack_sequence_as(structure=state_size,\n",
    "                                  flat_sequence=zeros_flat)\n",
    "  else:\n",
    "    zeros_size = _state_size_with_prefix(state_size, prefix=[batch_size])\n",
    "    zeros = array_ops.zeros(array_ops.stack(zeros_size), dtype=dtype)\n",
    "    zeros.set_shape(_state_size_with_prefix(state_size, prefix=[None]))\n",
    "\n",
    "  return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\n",
    "\n",
    "class LSTMStateTuple(_LSTMStateTuple):\n",
    "  \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n",
    "  Stores two elements: `(c, h)`, in that order.\n",
    "  Only used when `state_is_tuple=True`.\n",
    "  \"\"\"\n",
    "  __slots__ = ()\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    (c, h) = self\n",
    "    if not c.dtype == h.dtype:\n",
    "      raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
    "                      (str(c.dtype), str(h.dtype)))\n",
    "    return c.dtype\n",
    "\n",
    "\n",
    "class BasicLSTMCell(object):\n",
    "  \"\"\"Basic LSTM recurrent network cell.\n",
    "  The implementation is based on: http://arxiv.org/abs/1409.2329.\n",
    "  We add forget_bias (default: 1) to the biases of the forget gate in order to\n",
    "  reduce the scale of forgetting in the beginning of the training.\n",
    "  It does not allow cell clipping, a projection layer, and does not\n",
    "  use peep-hole connections: it is the basic baseline.\n",
    "  For advanced models, please use the full LSTMCell that follows.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_units, forget_bias=1.0, input_size=None,\n",
    "               state_is_tuple=True, activation=tf.tanh, reuse=None):\n",
    "    \"\"\"Initialize the basic LSTM cell.\n",
    "    Args:\n",
    "      num_units: int, The number of units in the LSTM cell.\n",
    "      forget_bias: float, The bias added to forget gates (see above).\n",
    "      input_size: Deprecated and unused.\n",
    "      state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "        the `c_state` and `m_state`.  If False, they are concatenated\n",
    "        along the column axis.  The latter behavior will soon be deprecated.\n",
    "      activation: Activation function of the inner states.\n",
    "      reuse: (optional) Python boolean describing whether to reuse variables\n",
    "        in an existing scope.  If not `True`, and the existing scope already has\n",
    "        the given variables, an error is raised.\n",
    "    \"\"\"\n",
    "    if not state_is_tuple:\n",
    "      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
    "                   \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "    if input_size is not None:\n",
    "      logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "    self._num_units = num_units\n",
    "    self._forget_bias = forget_bias\n",
    "    self._state_is_tuple = state_is_tuple\n",
    "    self._activation = activation\n",
    "    self._reuse = reuse\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (LSTMStateTuple(self._num_units, self._num_units)\n",
    "            if self._state_is_tuple else 2 * self._num_units)\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\n",
    "      # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "      if self._state_is_tuple:\n",
    "        c, h = state\n",
    "      else:\n",
    "        c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n",
    "      concat = _linear([inputs, h], 4 * self._num_units, True)\n",
    "\n",
    "      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "      i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1)\n",
    "\n",
    "      new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) *\n",
    "               self._activation(j))\n",
    "      new_h = self._activation(new_c) * sigmoid(o)\n",
    "\n",
    "      if self._state_is_tuple:\n",
    "        new_state = LSTMStateTuple(new_c, new_h)\n",
    "      else:\n",
    "        new_state = array_ops.concat([new_c, new_h], 1)\n",
    "      return new_h, new_state\n",
    "\n",
    "  def zero_state(self, batch_size, dtype):\n",
    "    \"\"\"Return zero-filled state tensor(s).\n",
    "    Args:\n",
    "      batch_size: int, float, or unit Tensor representing the batch size.\n",
    "      dtype: the data type to use for the state.\n",
    "    Returns:\n",
    "      If `state_size` is an int or TensorShape, then the return value is a\n",
    "      `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n",
    "      If `state_size` is a nested list or tuple, then the return value is\n",
    "      a nested list or tuple (of the same structure) of `2-D` tensors with\n",
    "      the shapes `[batch_size x s]` for each s in `state_size`.\n",
    "    \"\"\"\n",
    "    with ops.name_scope(type(self).__name__ + \"ZeroState\", values=[batch_size]):\n",
    "      state_size = self.state_size\n",
    "      return _zero_state_tensors(state_size, batch_size, dtype)\n",
    "\n",
    "\n",
    "class LSTMCell(object):\n",
    "  \"\"\"Long short-term memory unit (LSTM) recurrent network cell.\n",
    "  The default non-peephole implementation is based on:\n",
    "    http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "  S. Hochreiter and J. Schmidhuber.\n",
    "  \"Long Short-Term Memory\". Neural Computation, 9(8):1735-1780, 1997.\n",
    "  The peephole implementation is based on:\n",
    "    https://research.google.com/pubs/archive/43905.pdf\n",
    "  Hasim Sak, Andrew Senior, and Francoise Beaufays.\n",
    "  \"Long short-term memory recurrent neural network architectures for\n",
    "   large scale acoustic modeling.\" INTERSPEECH, 2014.\n",
    "  The class uses optional peep-hole connections, optional cell clipping, and\n",
    "  an optional projection layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_units, input_size=None,\n",
    "               use_peepholes=False, cell_clip=None,\n",
    "               initializer=None, num_proj=None, proj_clip=None,\n",
    "               num_unit_shards=None, num_proj_shards=None,\n",
    "               forget_bias=1.0, state_is_tuple=True,\n",
    "               activation=tf.tanh, reuse=None):\n",
    "    \"\"\"Initialize the parameters for an LSTM cell.\n",
    "    Args:\n",
    "      num_units: int, The number of units in the LSTM cell\n",
    "      input_size: Deprecated and unused.\n",
    "      use_peepholes: bool, set True to enable diagonal/peephole connections.\n",
    "      cell_clip: (optional) A float value, if provided the cell state is clipped\n",
    "        by this value prior to the cell output activation.\n",
    "      initializer: (optional) The initializer to use for the weight and\n",
    "        projection matrices.\n",
    "      num_proj: (optional) int, The output dimensionality for the projection\n",
    "        matrices.  If None, no projection is performed.\n",
    "      proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n",
    "        provided, then the projected values are clipped elementwise to within\n",
    "        `[-proj_clip, proj_clip]`.\n",
    "      num_unit_shards: Deprecated, will be removed by Jan. 2017.\n",
    "        Use a variable_scope partitioner instead.\n",
    "      num_proj_shards: Deprecated, will be removed by Jan. 2017.\n",
    "        Use a variable_scope partitioner instead.\n",
    "      forget_bias: Biases of the forget gate are initialized by default to 1\n",
    "        in order to reduce the scale of forgetting at the beginning of\n",
    "        the training.\n",
    "      state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "        the `c_state` and `m_state`.  If False, they are concatenated\n",
    "        along the column axis.  This latter behavior will soon be deprecated.\n",
    "      activation: Activation function of the inner states.\n",
    "      reuse: (optional) Python boolean describing whether to reuse variables\n",
    "        in an existing scope.  If not `True`, and the existing scope already has\n",
    "        the given variables, an error is raised.\n",
    "    \"\"\"\n",
    "    if not state_is_tuple:\n",
    "      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
    "                   \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "    if input_size is not None:\n",
    "      logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "    if num_unit_shards is not None or num_proj_shards is not None:\n",
    "      logging.warn(\n",
    "          \"%s: The num_unit_shards and proj_unit_shards parameters are \"\n",
    "          \"deprecated and will be removed in Jan 2017.  \"\n",
    "          \"Use a variable scope with a partitioner instead.\", self)\n",
    "\n",
    "    self._num_units = num_units\n",
    "    self._use_peepholes = use_peepholes\n",
    "    self._cell_clip = cell_clip\n",
    "    self._initializer = initializer\n",
    "    self._num_proj = num_proj\n",
    "    self._proj_clip = proj_clip\n",
    "    self._num_unit_shards = num_unit_shards\n",
    "    self._num_proj_shards = num_proj_shards\n",
    "    self._forget_bias = forget_bias\n",
    "    self._state_is_tuple = state_is_tuple\n",
    "    self._activation = activation\n",
    "    self._reuse = reuse\n",
    "\n",
    "    if num_proj:\n",
    "      self._state_size = (\n",
    "          LSTMStateTuple(num_units, num_proj)\n",
    "          if state_is_tuple else num_units + num_proj)\n",
    "      self._output_size = num_proj\n",
    "    else:\n",
    "      self._state_size = (\n",
    "          LSTMStateTuple(num_units, num_units)\n",
    "          if state_is_tuple else 2 * num_units)\n",
    "      self._output_size = num_units\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._state_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._output_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run one step of LSTM.\n",
    "    Args:\n",
    "      inputs: input Tensor, 2D, batch x num_units.\n",
    "      state: if `state_is_tuple` is False, this must be a state Tensor,\n",
    "        `2-D, batch x state_size`.  If `state_is_tuple` is True, this must be a\n",
    "        tuple of state Tensors, both `2-D`, with column sizes `c_state` and\n",
    "        `m_state`.\n",
    "      scope: VariableScope for the created subgraph; defaults to \"lstm_cell\".\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - A `2-D, [batch x output_dim]`, Tensor representing the output of the\n",
    "        LSTM after reading `inputs` when previous state was `state`.\n",
    "        Here output_dim is:\n",
    "           num_proj if num_proj was set,\n",
    "           num_units otherwise.\n",
    "      - Tensor(s) representing the new state of LSTM after reading `inputs` when\n",
    "        the previous state was `state`.  Same type and shape(s) as `state`.\n",
    "    Raises:\n",
    "      ValueError: If input size cannot be inferred from inputs via\n",
    "        static shape inference.\n",
    "    \"\"\"\n",
    "    num_proj = self._num_units if self._num_proj is None else self._num_proj\n",
    "\n",
    "    if self._state_is_tuple:\n",
    "      (c_prev, m_prev) = state\n",
    "    else:\n",
    "      c_prev = array_ops.slice(state, [0, 0], [-1, self._num_units])\n",
    "      m_prev = array_ops.slice(state, [0, self._num_units], [-1, num_proj])\n",
    "\n",
    "    dtype = inputs.dtype\n",
    "    input_size = inputs.get_shape().with_rank(2)[1]\n",
    "    if input_size.value is None:\n",
    "      raise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")\n",
    "    with _checked_scope(self, scope or \"lstm_cell\",\n",
    "                        initializer=self._initializer,\n",
    "                        reuse=self._reuse) as unit_scope:\n",
    "      if self._num_unit_shards is not None:\n",
    "        unit_scope.set_partitioner(\n",
    "            partitioned_variables.fixed_size_partitioner(\n",
    "                self._num_unit_shards))\n",
    "      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "      lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True)\n",
    "      i, j, f, o = array_ops.split(\n",
    "          value=lstm_matrix, num_or_size_splits=4, axis=1)\n",
    "      # Diagonal connections\n",
    "      if self._use_peepholes:\n",
    "        with vs.variable_scope(unit_scope) as projection_scope:\n",
    "          if self._num_unit_shards is not None:\n",
    "            projection_scope.set_partitioner(None)\n",
    "          w_f_diag = vs.get_variable(\n",
    "              \"w_f_diag\", shape=[self._num_units], dtype=dtype)\n",
    "          w_i_diag = vs.get_variable(\n",
    "              \"w_i_diag\", shape=[self._num_units], dtype=dtype)\n",
    "          w_o_diag = vs.get_variable(\n",
    "              \"w_o_diag\", shape=[self._num_units], dtype=dtype)\n",
    "\n",
    "      if self._use_peepholes:\n",
    "        c = (sigmoid(f + self._forget_bias + w_f_diag * c_prev) * c_prev +\n",
    "             sigmoid(i + w_i_diag * c_prev) * self._activation(j))\n",
    "      else:\n",
    "        c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *\n",
    "             self._activation(j))\n",
    "\n",
    "      if self._cell_clip is not None:\n",
    "        # pylint: disable=invalid-unary-operand-type\n",
    "        c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)\n",
    "        # pylint: enable=invalid-unary-operand-type\n",
    "      if self._use_peepholes:\n",
    "        m = sigmoid(o + w_o_diag * c) * self._activation(c)\n",
    "      else:\n",
    "        m = sigmoid(o) * self._activation(c)\n",
    "\n",
    "      if self._num_proj is not None:\n",
    "        with vs.variable_scope(\"projection\") as proj_scope:\n",
    "          if self._num_proj_shards is not None:\n",
    "            proj_scope.set_partitioner(\n",
    "                partitioned_variables.fixed_size_partitioner(\n",
    "                    self._num_proj_shards))\n",
    "          m = _linear(m, self._num_proj, bias=False)\n",
    "\n",
    "        if self._proj_clip is not None:\n",
    "          # pylint: disable=invalid-unary-operand-type\n",
    "          m = clip_ops.clip_by_value(m, -self._proj_clip, self._proj_clip)\n",
    "          # pylint: enable=invalid-unary-operand-type\n",
    "\n",
    "    new_state = (LSTMStateTuple(c, m) if self._state_is_tuple else\n",
    "                 array_ops.concat([c, m], 1))\n",
    "    return m, new_state\n",
    "\n",
    "  def zero_state(self, batch_size, dtype):\n",
    "    \"\"\"Return zero-filled state tensor(s).\n",
    "    Args:\n",
    "      batch_size: int, float, or unit Tensor representing the batch size.\n",
    "      dtype: the data type to use for the state.\n",
    "    Returns:\n",
    "      If `state_size` is an int or TensorShape, then the return value is a\n",
    "      `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.\n",
    "      If `state_size` is a nested list or tuple, then the return value is\n",
    "      a nested list or tuple (of the same structure) of `2-D` tensors with\n",
    "      the shapes `[batch_size x s]` for each s in `state_size`.\n",
    "    \"\"\"\n",
    "    with ops.name_scope(type(self).__name__ + \"ZeroState\", values=[batch_size]):\n",
    "      state_size = self.state_size\n",
    "      return _zero_state_tensors(state_size, batch_size, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
